{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Automated Data Import from Climate Data Store\n",
    "short_title: Automated Data Import\n",
    "---\n",
    "\n",
    "In this notebook we will demonstrate a full workflow for how we can use Climate Tools to automate regularly downloading data from the [Climate Data Store (CDS)](https://cds.climate.copernicus.eu/datasets), aggregating to DHIS2 organisation units, and importing the aggregated climate data back to DHIS2. \n",
    "\n",
    "For our example we will connect to a local DHIS2 instance containing the Sierra Leone demo database, setup new data elements for daily Temperature data and daily Total precipitation data, and show how to use `dhis2eo` to download, and import/update DHIS2 with the latest [daily data from the Climate Data Store ERA5 dataset](https://cds.climate.copernicus.eu/datasets/derived-era5-single-levels-daily-statistics?tab=download). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------\n",
    "## Requirements\n",
    "\n",
    "### 1. Connect to DHIS2\n",
    "\n",
    "In order to run this notebook, you first need to connect to an instance of DHIS2. For our example, we will connect to a local instance of DHIS2 containing the standard Sierra Leone demo database, but you should be able to switch out the instance url and credentials to work directly with your own database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current DHIS2 version: 2.42.2\n"
     ]
    }
   ],
   "source": [
    "from dhis2_client import DHIS2Client\n",
    "from dhis2_client.settings import ClientSettings\n",
    "\n",
    "# Create DHIS2 client connection\n",
    "cfg = ClientSettings(\n",
    "  base_url=\"http://localhost:8080\",\n",
    "  username=\"admin\",\n",
    "  password=\"district\"\n",
    ")\n",
    "client = DHIS2Client(settings=cfg)\n",
    "\n",
    "# Verify connection\n",
    "info = client.get_system_info()\n",
    "print(\"Current DHIS2 version:\", info[\"version\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create DHIS2 data elements\n",
    "\n",
    "We also need to create the data elements for importing data into. If you haven't already created your data elements manually, you can follow the steps below to create the data element using the `python-dhis2-client`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create the temperature data element: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data element creation status: OK and UID: gPPVvS6u23w\n"
     ]
    }
   ],
   "source": [
    "data_element = {\n",
    "    \"name\": \"2m Temperature (ERA5)\",\n",
    "    \"shortName\": \"Temperature (ERA5)\",\n",
    "    \"valueType\": \"NUMBER\",\n",
    "    \"aggregationType\": \"AVERAGE\",\n",
    "    \"domainType\": \"AGGREGATE\"\n",
    "}\n",
    "temperature_de = client.create_data_element(data_element)\n",
    "print(f\"Data element creation status: {temperature_de['status']} and UID: {temperature_de['response']['uid']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the total precipitation data element: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data element creation status: OK and UID: i9W7DhW60kK\n"
     ]
    }
   ],
   "source": [
    "data_element = {\n",
    "    \"name\": \"Total precipitation (ERA5)\",\n",
    "    \"shortName\": \"Total precipitation (ERA5)\",\n",
    "    \"valueType\": \"NUMBER\",\n",
    "    \"aggregationType\": \"SUM\",\n",
    "    \"domainType\": \"AGGREGATE\"\n",
    "}\n",
    "precipitation_de = client.create_data_element(data_element)\n",
    "print(f\"Data element creation status: {precipitation_de['status']} and UID: {precipitation_de['response']['uid']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we plan to import daily data values, we also create and assign our data element to a new dataset for climate variables with `Daily` period type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set creation status: OK and UID: SwDpDu9fq4H\n"
     ]
    }
   ],
   "source": [
    "data_set = {\n",
    "    \"name\": \"Daily climate data\", \n",
    "    \"shortName\": \"Daily climate data\",\n",
    "    \"periodType\": \"Daily\",\n",
    "    \"dataSetElements\": [\n",
    "        {\n",
    "            \"dataElement\": {\"id\": temperature_de['response']['uid']},\n",
    "            \"dataElement\": {\"id\": precipitation_de['response']['uid']}\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "data_set_response = client.create_data_set(data_set)\n",
    "print(f\"Data set creation status: {data_set_response['status']} and UID: {data_set_response['response']['uid']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Register for ERA5 Data Access\n",
    "\n",
    "#### Authenticate with your ECMWF user\n",
    "\n",
    "Before you can download the dataset programmatically, you need to [create an ECMWF user](https://www.ecmwf.int/user/login), and authenticate using your user credentials:\n",
    "\n",
    "- Go to the [CDSAPI Setup page](https://cds.climate.copernicus.eu/how-to-api) and make sure to login.\n",
    "- Once logged in, scroll down to the section \"Setup the CDS API personal access token\". \n",
    "  - This should show your login credentials, and look something like this:\n",
    "\n",
    "        url: https://cds.climate.copernicus.eu/api\n",
    "        key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "\n",
    "- Copy those two lines to a file `.cdsapirc` in your user's $HOME directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accept the dataset license\n",
    "\n",
    "ECMWF requires that you manually accept the user license for each dataset that you download. \n",
    "\n",
    "- Start by visiting the Download page of the dataset we are interested in: [\"ERA5 post-processed daily statistics on single levels from 1940 to present\"](https://cds.climate.copernicus.eu/datasets/derived-era5-single-levels-daily-statistics?tab=download). \n",
    "- Scroll down until you get to the \"Terms of Use\" section.\n",
    "- Click the button to accept and login with your user if you haven't already. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "## Quickstart: Keeping DHIS2 up to date with ERA5 climate data\n",
    "\n",
    "In many cases, users will often want to download and import a standard set of climate variables from ERA5 into DHIS2. For this reason, Climate Tools provides a simple way to synch DHIS2 with ERA5 climate data for a defined time period. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to run the function to import daily ERA5 data since 1 June 2025 until today: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dhis2eo\n",
    "import dhis2eo.org_units\n",
    "import dhis2eo.data.cds\n",
    "import dhis2eo.synch\n",
    "import dhis2eo.utils.earthkit\n",
    "\n",
    "# define which data element ids should import which data variables\n",
    "# TODO: these should probably be created per test and maybe initialized with some test values...\n",
    "data_elements_to_variables = {'gPPVvS6u23w': 't2m', 'i9W7DhW60kK': 'tp'}\n",
    "\n",
    "def download_func(year, month, org_units):\n",
    "    data = dhis2eo.data.cds.get_daily_era5_data(year, month, org_units)\n",
    "    return data\n",
    "\n",
    "# run the synch function\n",
    "org_unit_level = 2\n",
    "start_year = 2025\n",
    "start_month = 6\n",
    "dhis2eo.synch.synch_dhis2_data(\n",
    "    client, \n",
    "    download_func,\n",
    "    start_year,\n",
    "    start_month,\n",
    "    data_elements_to_variables,\n",
    "    org_unit_level=org_unit_level,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Running this data import function multiple times in the same month, will result in the entire month being downloaded and imported each time, since the data is updated on a daily basis. But the results from the data import will report how many data values already existed and were ignored, and how many new data values were imported since last time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "## Custom workflows: Importing ERA5 data into DHIS2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous convenience function for For importing data from the Climate Data Store (CDS), we will demonstrate step-by-step how to use the `dhsi2eo` package to programmatically retreive the data from the CDS API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Retrieve organisation units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can download the data, we first need to load our organisation units in order to limit which region to download data for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we retrieve the organisation units as a GeoJSON dict from the `dhis2-python-client`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_units_geojson = client.get_org_units_geojson(level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load this GeoJSON dict as a `geopandas.GeoDataFrame` by using the `dhis2eo.org_units` module. This makes it easier work with the organisation units for later steps: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    org_unit_id          name  \\\n",
      "0   O6uvpzGd5pu            Bo   \n",
      "1   fdc6uOvgoji       Bombali   \n",
      "2   lc3eMKXaEfw        Bonthe   \n",
      "3   jUb8gELQApl      Kailahun   \n",
      "4   PMa2VCrupOd        Kambia   \n",
      "5   kJq2mPyFEHo        Kenema   \n",
      "6   qhqAxPSTUXp     Koinadugu   \n",
      "7   Vth0fbpFcsO          Kono   \n",
      "8   jmIPBj66vD6       Moyamba   \n",
      "9   TEQlaapDQoK     Port Loko   \n",
      "10  bL4ooGhyHRQ       Pujehun   \n",
      "11  eIQbndfxQMb     Tonkolili   \n",
      "12  at6UHUQatSo  Western Area   \n",
      "\n",
      "                                             geometry  \n",
      "0   POLYGON ((-11.5914 8.4875, -11.5906 8.4769, -1...  \n",
      "1   POLYGON ((-11.8091 9.2032, -11.8102 9.1944, -1...  \n",
      "2   MULTIPOLYGON (((-12.5568 7.3832, -12.5574 7.38...  \n",
      "3   POLYGON ((-10.7972 7.5866, -10.8002 7.5878, -1...  \n",
      "4   MULTIPOLYGON (((-13.1349 8.8471, -13.1343 8.84...  \n",
      "5   POLYGON ((-11.3596 8.5317, -11.3513 8.5234, -1...  \n",
      "6   POLYGON ((-10.585 9.0434, -10.5877 9.0432, -10...  \n",
      "7   POLYGON ((-10.585 9.0434, -10.5848 9.0432, -10...  \n",
      "8   MULTIPOLYGON (((-12.6351 7.6613, -12.6346 7.66...  \n",
      "9   MULTIPOLYGON (((-13.119 8.4718, -13.1174 8.470...  \n",
      "10  MULTIPOLYGON (((-11.5346 6.944, -11.5338 6.943...  \n",
      "11  POLYGON ((-11.3546 8.6455, -11.357 8.6345, -11...  \n",
      "12  MULTIPOLYGON (((-13.2435 8.1007, -13.2429 8.10...  \n"
     ]
    }
   ],
   "source": [
    "org_units = dhis2eo.org_units.from_dhis2_geojson(org_units_geojson)\n",
    "print(org_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Download daily ERA5 data\n",
    "\n",
    "In order to get users started, we provide a convenience function for downloading the most commonly requested climate variables from the [ERA5 post-processed daily statistics on single levels from 1940 to present](https://cds.climate.copernicus.eu/datasets/derived-era5-single-levels-daily-statistics). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply provide the year, month, and org_units you want to download for. The region to download data for is automatically calculated from the provided organisation units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhis2eo.data.cds - INFO - Loading from cache: C:\\Users\\karimba\\AppData\\Local\\Temp\\cds_daily-era5_params-ca5bab_region-37098a_2021-01.nc\n"
     ]
    }
   ],
   "source": [
    "data = dhis2eo.data.cds.get_daily_era5_data(2021, 1, org_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since data downloads can be slow, this function also caches the download results and reuses it if the file has already been downloaded. Calling it again will be much faster by loading directly from the cache: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhis2eo.data.cds - INFO - Loading from cache: C:\\Users\\karimba\\AppData\\Local\\Temp\\cds_daily-era5_params-ca5bab_region-37098a_2021-01.nc\n"
     ]
    }
   ],
   "source": [
    "data = dhis2eo.data.cds.get_daily_era5_data(2021, 1, org_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: since we are working with a daily updated dataset, the function only caches downloads for past months, but not for the current month. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data processing\n",
    "\n",
    "After downloading the data, we might also want to do some processing of the climate data to get the values or units that we want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Aggregate the data to organisation units\n",
    "\n",
    "The next step is using the `aggregate_to_org_units` convenience function to aggregate the climate data to a set of input organisation units. Let's try it for our previously downloaded test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    valid_time  org_unit_id  number        t2m\n",
      "0   2012-02-02  O6uvpzGd5pu       0  27.335388\n",
      "1   2012-02-02  fdc6uOvgoji       0  27.853302\n",
      "2   2012-02-02  lc3eMKXaEfw       0  26.812439\n",
      "3   2012-02-02  jUb8gELQApl       0  27.563202\n",
      "4   2012-02-02  PMa2VCrupOd       0  27.259949\n",
      "..         ...          ...     ...        ...\n",
      "346 2012-02-28  jmIPBj66vD6       0  26.785797\n",
      "347 2012-02-28  TEQlaapDQoK       0  27.028656\n",
      "348 2012-02-28  bL4ooGhyHRQ       0  26.023712\n",
      "349 2012-02-28  eIQbndfxQMb       0  27.649323\n",
      "350 2012-02-28  at6UHUQatSo       0        NaN\n",
      "\n",
      "[351 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "agg = dhis2eo.utils.earthkit.aggregate_to_org_units(data, org_units)\n",
    "print(agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the aggregated data contains temperature values for each organisation unit (`org_unit_id`) and all the 28 days in February 2012 contained in the downloaded NetCDF data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Synch data with DHIS2 time periods\n",
    "\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month_synch_status in dhis2eo.synch.iter_dhis2_monthly_synch_status(client, start_year, start_month, data_element_ids, org_unit_level):\n",
    "    print(month_synch_status)\n",
    "    # Download data...\n",
    "    # Aggregate data...\n",
    "    # Convert and import to DHIS2..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "In this notebook we have shown how to use `dhis2eo` convenience functions for automatically synching ERA5 climate data into DHIS2. This function can be run at regular intervals, e.g. every day or week, to fetch and import only the latest temperature data for your org units. But we still need a way to run the script. This can be done either manually, or automatically via a `cron` job. Further guidance on how to automatically schedule running a script will be added in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate-tools-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
