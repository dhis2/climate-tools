{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Automated Data Import from Climate Data Store\n",
    "short_title: Automated Data Import\n",
    "---\n",
    "\n",
    "In this notebook we will demonstrate a full workflow for how we can use Climate Tools to automate regularly downloading data from the [Climate Data Store (CDS)](https://cds.climate.copernicus.eu/datasets), aggregating to DHIS2 organisation units, and importing the aggregated climate data back to DHIS2. \n",
    "\n",
    "For our example we will connect to a local DHIS2 instance containing the Sierra Leone demo database, setup new data elements for daily Temperature data and daily Total precipitation data, and show how to use `dhis2eo` to download, and import/update DHIS2 with the latest [daily data from the Climate Data Store ERA5 dataset](https://cds.climate.copernicus.eu/datasets/derived-era5-single-levels-daily-statistics?tab=download). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------\n",
    "## Requirements\n",
    "\n",
    "In order to run this notebook, you first need to connect to an instance of DHIS2. For our example, we will connect to a local instance of DHIS2 containing the standard Sierra Leone demo database, but you should be able to switch out the instance url and credentials to work directly with your own database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current DHIS2 version: 2.42.2\n"
     ]
    }
   ],
   "source": [
    "from dhis2_client import DHIS2Client\n",
    "from dhis2_client.settings import ClientSettings\n",
    "\n",
    "# Create DHIS2 client connection\n",
    "cfg = ClientSettings(\n",
    "  base_url=\"http://localhost:8080\",\n",
    "  username=\"admin\",\n",
    "  password=\"district\"\n",
    ")\n",
    "client = DHIS2Client(settings=cfg)\n",
    "\n",
    "# Verify connection\n",
    "info = client.get_system_info()\n",
    "print(\"Current DHIS2 version:\", info[\"version\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to create the data elements for importing data into. If you haven't already created your data elements manually, you can follow the steps below to create the data element using the `python-dhis2-client`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create the temperature data element: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data element creation status: OK and UID: gPPVvS6u23w\n"
     ]
    }
   ],
   "source": [
    "data_element = {\n",
    "    \"name\": \"2m Temperature (ERA5)\",\n",
    "    \"shortName\": \"Temperature (ERA5)\",\n",
    "    \"valueType\": \"NUMBER\",\n",
    "    \"aggregationType\": \"AVERAGE\",\n",
    "    \"domainType\": \"AGGREGATE\"\n",
    "}\n",
    "temperature_de = client.create_data_element(data_element)\n",
    "print(f\"Data element creation status: {temperature_de['status']} and UID: {temperature_de['response']['uid']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the total precipitation data element: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data element creation status: OK and UID: i9W7DhW60kK\n"
     ]
    }
   ],
   "source": [
    "data_element = {\n",
    "    \"name\": \"Total precipitation (ERA5)\",\n",
    "    \"shortName\": \"Total precipitation (ERA5)\",\n",
    "    \"valueType\": \"NUMBER\",\n",
    "    \"aggregationType\": \"SUM\",\n",
    "    \"domainType\": \"AGGREGATE\"\n",
    "}\n",
    "precipitation_de = client.create_data_element(data_element)\n",
    "print(f\"Data element creation status: {precipitation_de['status']} and UID: {precipitation_de['response']['uid']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we plan to import daily data values, we also create and assign our data element to a new dataset for climate variables with `Daily` period type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set creation status: OK and UID: hAegfkyGjuu\n"
     ]
    }
   ],
   "source": [
    "data_set = {\n",
    "    \"name\": \"Daily climate data\", \n",
    "    \"shortName\": \"Daily climate data\",\n",
    "    \"periodType\": \"Daily\",\n",
    "    \"dataSetElements\": [\n",
    "        {\n",
    "            \"dataElement\": {\"id\": temperature_de['response']['uid']},\n",
    "            \"dataElement\": {\"id\": precipitation_de['response']['uid']}\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "data_set_response = client.create_data_set(data_set)\n",
    "print(f\"Data set creation status: {data_set_response['status']} and UID: {data_set_response['response']['uid']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "## Downloading CDS data for a given month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For downloading data from the Climate Data Store (CDS), we will demonstrate step-by-step how to use the `dhsi2eo` package to programmatically retreive the data from the CDS API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dhis2eo\n",
    "import dhis2eo.org_units\n",
    "import dhis2eo.data.cds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "#### 1. Authenticate with your ECMWF user\n",
    "\n",
    "Before you can download the dataset programmatically, you need to [create an ECMWF user](https://www.ecmwf.int/user/login), and authenticate using your user credentials:\n",
    "\n",
    "- Go to the [CDSAPI Setup page](https://cds.climate.copernicus.eu/how-to-api) and make sure to login.\n",
    "- Once logged in, scroll down to the section \"Setup the CDS API personal access token\". \n",
    "  - This should show your login credentials, and look something like this:\n",
    "\n",
    "        url: https://cds.climate.copernicus.eu/api\n",
    "        key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "\n",
    "- Copy those two lines to a file `.cdsapirc` in your user's $HOME directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Accept the dataset license\n",
    "\n",
    "ECMWF requires that you manually accept the user license for each dataset that you download. \n",
    "\n",
    "- Start by visiting the Download page of the dataset we are interested in: [\"ERA5 post-processed daily statistics on single levels from 1940 to present\"](https://cds.climate.copernicus.eu/datasets/derived-era5-single-levels-daily-statistics?tab=download). \n",
    "- Scroll down until you get to the \"Terms of Use\" section.\n",
    "- Click the button to accept and login with your user if you haven't already. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving organisation units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can download the data, we first need to load our organisation units in order to limit which region to download data for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we retrieve the organisation units as a GeoJSON dict from the `dhis2-python-client`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_units_geojson = client.get_org_units_geojson(level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load this GeoJSON dict as a `geopandas.GeoDataFrame` by using the `dhis2eo.org_units` module. This makes it easier work with the organisation units for later steps: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    org_unit_id          name  \\\n",
      "0   O6uvpzGd5pu            Bo   \n",
      "1   fdc6uOvgoji       Bombali   \n",
      "2   lc3eMKXaEfw        Bonthe   \n",
      "3   jUb8gELQApl      Kailahun   \n",
      "4   PMa2VCrupOd        Kambia   \n",
      "5   kJq2mPyFEHo        Kenema   \n",
      "6   qhqAxPSTUXp     Koinadugu   \n",
      "7   Vth0fbpFcsO          Kono   \n",
      "8   jmIPBj66vD6       Moyamba   \n",
      "9   TEQlaapDQoK     Port Loko   \n",
      "10  bL4ooGhyHRQ       Pujehun   \n",
      "11  eIQbndfxQMb     Tonkolili   \n",
      "12  at6UHUQatSo  Western Area   \n",
      "\n",
      "                                             geometry  \n",
      "0   POLYGON ((-11.5914 8.4875, -11.5906 8.4769, -1...  \n",
      "1   POLYGON ((-11.8091 9.2032, -11.8102 9.1944, -1...  \n",
      "2   MULTIPOLYGON (((-12.5568 7.3832, -12.5574 7.38...  \n",
      "3   POLYGON ((-10.7972 7.5866, -10.8002 7.5878, -1...  \n",
      "4   MULTIPOLYGON (((-13.1349 8.8471, -13.1343 8.84...  \n",
      "5   POLYGON ((-11.3596 8.5317, -11.3513 8.5234, -1...  \n",
      "6   POLYGON ((-10.585 9.0434, -10.5877 9.0432, -10...  \n",
      "7   POLYGON ((-10.585 9.0434, -10.5848 9.0432, -10...  \n",
      "8   MULTIPOLYGON (((-12.6351 7.6613, -12.6346 7.66...  \n",
      "9   MULTIPOLYGON (((-13.119 8.4718, -13.1174 8.470...  \n",
      "10  MULTIPOLYGON (((-11.5346 6.944, -11.5338 6.943...  \n",
      "11  POLYGON ((-11.3546 8.6455, -11.357 8.6345, -11...  \n",
      "12  MULTIPOLYGON (((-13.2435 8.1007, -13.2429 8.10...  \n"
     ]
    }
   ],
   "source": [
    "org_units = dhis2eo.org_units.from_dhis2_geojson(org_units_geojson)\n",
    "print(org_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading daily ERA5 data\n",
    "\n",
    "In order to get users started, we provide a convenience function for downloading the most commonly requested climate variables from the [ERA5 post-processed daily statistics on single levels from 1940 to present](https://cds.climate.copernicus.eu/datasets/derived-era5-single-levels-daily-statistics). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply provide the year, month, and org_units you want to download for. The region to download data for is automatically calculated from the provided organisation units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhis2eo.data.cds - INFO - Loading from cache: C:\\Users\\karimba\\AppData\\Local\\Temp\\cds_daily-era5_params-ca5bab_region-37098a_2021-01.nc\n"
     ]
    }
   ],
   "source": [
    "data = dhis2eo.data.cds.get_daily_era5_data(2021, 1, org_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since data downloads can be slow, this function also caches the download results and reuses it if the file has already been downloaded. Calling it again will be much faster by loading directly from the cache: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhis2eo.data.cds - INFO - Loading from cache: C:\\Users\\karimba\\AppData\\Local\\Temp\\cds_daily-era5_params-ca5bab_region-37098a_2021-01.nc\n"
     ]
    }
   ],
   "source": [
    "data = dhis2eo.data.cds.get_daily_era5_data(2021, 1, org_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: that since we are working with a daily updated dataset, the function only caches downloads for past months, but not for the current month. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS IS HOW FAR IVE REACHED..............."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------\n",
    "\n",
    "## Aggregating the data to organisation units\n",
    "\n",
    "The next step is creating a generic function that aggregates the data downloaded from the previous step to a set of input organisation units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(data, org_units, id_col):\n",
    "    from earthkit.transforms import aggregate\n",
    "    # aggregate to org unit for each time period\n",
    "    agg_data = aggregate.spatial.reduce(data, org_units, mask_dim=id_col)\n",
    "    # convert to dataframe\n",
    "    agg_df = agg_data.to_dataframe().reset_index()\n",
    "    # return\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it for our previously downloaded test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    valid_time  org_unit_id  number        t2m\n",
      "0   2012-02-02  O6uvpzGd5pu       0  27.335388\n",
      "1   2012-02-02  fdc6uOvgoji       0  27.853302\n",
      "2   2012-02-02  lc3eMKXaEfw       0  26.812439\n",
      "3   2012-02-02  jUb8gELQApl       0  27.563202\n",
      "4   2012-02-02  PMa2VCrupOd       0  27.259949\n",
      "..         ...          ...     ...        ...\n",
      "346 2012-02-28  jmIPBj66vD6       0  26.785797\n",
      "347 2012-02-28  TEQlaapDQoK       0  27.028656\n",
      "348 2012-02-28  bL4ooGhyHRQ       0  26.023712\n",
      "349 2012-02-28  eIQbndfxQMb       0  27.649323\n",
      "350 2012-02-28  at6UHUQatSo       0        NaN\n",
      "\n",
      "[351 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "agg = aggregate(data, org_units, id_col='org_unit_id')\n",
    "agg['t2m'] -= 273.15 # convert to celsius\n",
    "print(agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the aggregated data contains temperature values for each organisation unit (`org_unit_id`) and all the 28 days in February 2012 contained in the downloaded NetCDF data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------\n",
    "\n",
    "## Determining the data period for importing\n",
    "\n",
    "Now that we have a simple way to download and aggregate temperature data, we want a function that defines a time period for which we want data. We have two goals here:\n",
    "\n",
    "1. Since data is downloaded and processed on a monthly basis we want to return which year-month period we want to process. \n",
    "2. Return all year-month periods between the last valid data value for a given data element and today's date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_month_periods_since_last_data_value(data_element_id, earliest_year, earliest_month):\n",
    "    from datetime import date\n",
    "    # get current year and month\n",
    "    current_date = date.today()\n",
    "    current_year,current_month = current_date.year, current_date.month\n",
    "    # get last year and month for which data values exist in dhis2 data element\n",
    "    first_period_response = {'existing': None} # TODO: update once daily periods are supported # client.analytics_latest_period_for_level(de_uid=data_element_id, level=org_unit_level)\n",
    "    if first_period_response['existing']: \n",
    "        # last data value found\n",
    "        first_period = first_period_response['existing']['id']\n",
    "        first_year,first_month = int(first_period[:4]), int(first_period[4:6])\n",
    "        # but no earlier than earliest year-month\n",
    "        first_year = max(earliest_year, first_year)\n",
    "        first_month = max(earliest_month, first_month)\n",
    "    else:\n",
    "        # no data values exists, start at earliest year-month\n",
    "        first_year,first_month = earliest_year,earliest_month\n",
    "    # loop years and months between last dhis2 value and today's date\n",
    "    for year in range(first_year, current_year + 1):\n",
    "        start_month = first_month if year == first_year else 1\n",
    "        end_month = current_month if year == current_year else 12\n",
    "        for month in range(start_month, end_month + 1):\n",
    "            # yield year-month pairs\n",
    "            yield year,month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "## The full workflow\n",
    "\n",
    "Now we have all the components needed to automatically download data from the Climate Data Store. In this last section we will tie all the pieces together into a single function, which we can use to easily perform the data import at regular intervals: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_element_id, earliest_year, earliest_month):\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    from dhis2eo.integrations.pandas import dataframe_to_dhis2_json\n",
    "\n",
    "    # download org units geojson\n",
    "    print('Getting organisation units...')\n",
    "    geojson = client.get_org_units_geojson(level=org_unit_level)\n",
    "\n",
    "    # add org unit id to properties\n",
    "    for feat in geojson['features']:\n",
    "        feat['properties']['org_unit_id'] = feat['id']\n",
    "\n",
    "    # convert to geopandas and get bbox\n",
    "    org_units = gpd.GeoDataFrame.from_features(geojson[\"features\"])\n",
    "    bbox = get_bbox(org_units)\n",
    "\n",
    "    # fetch, aggregate, and import data month-by-month\n",
    "    for year, month in iter_month_periods_since_last_data_value(data_element_id, earliest_year, earliest_month):\n",
    "        print('====================')\n",
    "        print('Period:', year, month)\n",
    "        # download data\n",
    "        print('Getting data...')\n",
    "        data = get_temperature_data(year, month, bbox)\n",
    "        # aggregate to org units\n",
    "        print('Aggregating...')\n",
    "        agg = aggregate(data, org_units, id_col='org_unit_id')\n",
    "        # convert to celsius\n",
    "        agg['t2m'] -= 273.15\n",
    "        # ignore nan values\n",
    "        agg = agg[~pd.isna(agg['t2m'])]\n",
    "        # convert to dhis2 json\n",
    "        payload = dataframe_to_dhis2_json(\n",
    "            df=agg,\n",
    "            org_unit_col='org_unit_id',\n",
    "            period_col='valid_time',\n",
    "            value_col='t2m',\n",
    "            data_element_id=data_element_id,\n",
    "        )\n",
    "        # upload to dhis2\n",
    "        print('Importing to DHIS2...')\n",
    "        res = client.post(\"/api/dataValueSets\", json=payload)\n",
    "        print(\"Results:\", res['response']['importCount'])\n",
    "\n",
    "    print('=====================')\n",
    "    print('Data import finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try to run the function to import daily temperature data since 1 January 2025 until today: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting organisation units...\n",
      "====================\n",
      "Period: 2025 1\n",
      "Getting data...\n",
      "Loading from cache ../data/local\\temperature_2025-01_bbox_-13_6_-10_10.nc\n",
      "Aggregating...\n",
      "Importing to DHIS2...\n",
      "Results: {'imported': 0, 'updated': 348, 'ignored': 0, 'deleted': 0}\n",
      "====================\n",
      "Period: 2025 2\n",
      "Getting data...\n",
      "Loading from cache ../data/local\\temperature_2025-02_bbox_-13_6_-10_10.nc\n",
      "Aggregating...\n",
      "Importing to DHIS2...\n",
      "Results: {'imported': 0, 'updated': 276, 'ignored': 0, 'deleted': 0}\n",
      "====================\n",
      "Period: 2025 3\n",
      "Getting data...\n",
      "Loading from cache ../data/local\\temperature_2025-03_bbox_-13_6_-10_10.nc\n",
      "Aggregating...\n",
      "Importing to DHIS2...\n",
      "Results: {'imported': 0, 'updated': 312, 'ignored': 0, 'deleted': 0}\n",
      "====================\n",
      "Period: 2025 4\n",
      "Getting data...\n",
      "Loading from cache ../data/local\\temperature_2025-04_bbox_-13_6_-10_10.nc\n",
      "Aggregating...\n",
      "Importing to DHIS2...\n",
      "Results: {'imported': 0, 'updated': 348, 'ignored': 0, 'deleted': 0}\n",
      "====================\n",
      "Period: 2025 5\n",
      "Getting data...\n",
      "Loading from cache ../data/local\\temperature_2025-05_bbox_-13_6_-10_10.nc\n",
      "Aggregating...\n",
      "Importing to DHIS2...\n",
      "Results: {'imported': 0, 'updated': 336, 'ignored': 0, 'deleted': 0}\n",
      "====================\n",
      "Period: 2025 6\n",
      "Getting data...\n",
      "Loading from cache ../data/local\\temperature_2025-06_bbox_-13_6_-10_10.nc\n",
      "Aggregating...\n",
      "Importing to DHIS2...\n",
      "Results: {'imported': 0, 'updated': 288, 'ignored': 0, 'deleted': 0}\n",
      "====================\n",
      "Period: 2025 7\n",
      "Getting data...\n",
      "Loading from cache ../data/local\\temperature_2025-07_bbox_-13_6_-10_10.nc\n",
      "Aggregating...\n",
      "Importing to DHIS2...\n",
      "Results: {'imported': 0, 'updated': 360, 'ignored': 0, 'deleted': 0}\n",
      "====================\n",
      "Period: 2025 8\n",
      "Getting data...\n",
      "Loading from cache ../data/local\\temperature_2025-08_bbox_-13_6_-10_10.nc\n",
      "Aggregating...\n",
      "Importing to DHIS2...\n",
      "Results: {'imported': 0, 'updated': 324, 'ignored': 0, 'deleted': 0}\n",
      "====================\n",
      "Period: 2025 9\n",
      "Getting data...\n",
      "Loading from cache ../data/local\\temperature_2025-09_bbox_-13_6_-10_10.nc\n",
      "Aggregating...\n",
      "Importing to DHIS2...\n",
      "Results: {'imported': 0, 'updated': 348, 'ignored': 0, 'deleted': 0}\n",
      "====================\n",
      "Period: 2025 10\n",
      "Getting data...\n",
      "Downloading from api...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 08:59:01,894 INFO Request ID is c8242a40-9ded-41e2-ab18-ee811e8edb0f\n",
      "2025-10-09 08:59:01,982 INFO status has been updated to accepted\n",
      "2025-10-09 08:59:10,703 INFO status has been updated to running\n",
      "2025-10-09 08:59:23,579 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ede7f7f6184a99a882aa39a194a8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "7e36e70de28fdff87153c77cd2a5d634.nc:   0%|          | 0.00/22.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is for the current month and will not be cached, since data is added daily\n",
      "Aggregating...\n",
      "Importing to DHIS2...\n",
      "Results: {'imported': 0, 'updated': 24, 'ignored': 0, 'deleted': 0}\n",
      "=====================\n",
      "Data import finished!\n"
     ]
    }
   ],
   "source": [
    "data_element_id = 'GbUpvHzCzn8' # data element id that you want to import data into\n",
    "start_year = 2025\n",
    "start_month = 1\n",
    "main(data_element_id, start_year, start_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Running this data import function multiple times in the same month, will result in the entire month being downloaded and imported each time, since the data is updated on a daily basis. But the results from the data import will report how many data values already existed and were ignored, and how many new data values were imported since last time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "In this notebook we have created a function that can be run at regular intervals, e.g. every day or week, to fetch and import only the latest temperature data for your org units. But we still need a way to run the script. This can be done either manually, or automatically via a `cron` job. Further guidance on how to automatically schedule running a script will be added in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate-tools-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
