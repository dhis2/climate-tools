{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Data Download from Climate Data Store\n",
    "\n",
    "In this notebook we will demonstrate a full workflow for how we can use Climate Tools to automate regularly downloading data from the [Climate Data Store (CDS)](https://cds.climate.copernicus.eu/datasets), aggregating to DHIS2 organisation units, and uploading the aggregated climate data back to DHIS2. \n",
    "\n",
    "For our example we will connect to a local DHIS2 instance containing the Sierra Leone demo database, setup a new data element for daily Temperature data, and show how to create a function that can be called at regular intervals in order to update DHIS2 with the latest [daily 2m temperature data from the Climate Data Store](https://cds.climate.copernicus.eu/datasets/derived-era5-single-levels-daily-statistics?tab=download). \n",
    "\n",
    "Each of the steps will be explained in detail throughout the notebook. At the end of the notebook, we will tie it all together in a single code snippet that automatically checks what data has already been imported, and downloads and imports only the relevant data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------\n",
    "## Requirements\n",
    "\n",
    "In order to run this notebook, you first need to connect to an instance of DHIS2. For our example, we will connect to a local instance of DHIS2 containing the standard Sierra Leone demo database, but you should be able to switch out the instance url and credentials to work directly with your own database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current DHIS2 version: 2.42.2\n"
     ]
    }
   ],
   "source": [
    "from dhis2_client import DHIS2Client\n",
    "from dhis2_client.settings import ClientSettings\n",
    "\n",
    "# Create DHIS2 client connection\n",
    "cfg = ClientSettings(\n",
    "  base_url=\"http://localhost:8080\",\n",
    "  username=\"admin\",\n",
    "  password=\"district\"\n",
    ")\n",
    "client = DHIS2Client(settings=cfg)\n",
    "\n",
    "# Verify connection\n",
    "info = client.get_system_info()\n",
    "print(\"Current DHIS2 version:\", info[\"version\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to create the data element for importing data into. If you haven't already created your data element manually, you can follow the steps below to create the data element using the `python-dhis2-client`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data element creation status: OK and UID: GbUpvHzCzn8\n"
     ]
    }
   ],
   "source": [
    "data_element = {\n",
    "    \"name\": \"2m Temperature (ERA5)\",\n",
    "    \"shortName\": \"Temperature (ERA5)\",\n",
    "    \"valueType\": \"NUMBER\",\n",
    "    \"aggregationType\": \"AVERAGE\",\n",
    "    \"domainType\": \"AGGREGATE\"\n",
    "}\n",
    "data_element_response = client.create_data_element(data_element)\n",
    "print(f\"Data element creation status: {data_element_response['status']} and UID: {data_element_response['response']['uid']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we plan to import daily temperature values, we also create and assign our data element to a new dataset for climate variables with `Daily` period type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set creation status: OK and UID: MMPdeGYhikN\n"
     ]
    }
   ],
   "source": [
    "data_set = {\n",
    "    \"name\": \"Daily climate data\", \n",
    "    \"shortName\": \"Daily climate data\",\n",
    "    \"periodType\": \"Daily\",\n",
    "    \"dataSetElements\": [\n",
    "        {\n",
    "            \"dataElement\": {\"id\": data_element_response['response']['uid']}\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "data_set_response = client.create_data_set(data_set)\n",
    "print(f\"Data set creation status: {data_set_response['status']} and UID: {data_set_response['response']['uid']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "## Downloading CDS data for a given month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For downloading data from the Climate Data Store (CDS), we will demonstrate step-by-step how to use the `earthkit.data` package to programmatically retreive the data from the CDS API. For more information about CDS data access, see our guide for [manually downloading CDS data](../getting-data/climate-data-store.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthkit.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "#### 1. Authenticate with your ECMWF user\n",
    "\n",
    "Before you can download the dataset programmatically, you need to [create an ECMWF user](https://www.ecmwf.int/user/login), and authenticate using your user credentials:\n",
    "\n",
    "- Go to the [CDSAPI Setup page](https://cds.climate.copernicus.eu/how-to-api) and make sure to login.\n",
    "- Once logged in, scroll down to the section \"Setup the CDS API personal access token\". \n",
    "  - This should show your login credentials, and look something like this:\n",
    "\n",
    "        url: https://cds.climate.copernicus.eu/api\n",
    "        key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "\n",
    "- Copy those two lines to a file `.cdsapirc` in your user's $HOME directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Accept the dataset license\n",
    "\n",
    "ECMWF requires that you manually accept the user license for each dataset that you download. \n",
    "\n",
    "- Start by visiting the Download page of the dataset we are interested in: [\"ERA5 post-processed daily statistics on single levels from 1940 to present\"](https://cds.climate.copernicus.eu/datasets/derived-era5-single-levels-daily-statistics?tab=download). \n",
    "- Scroll down until you get to the \"Terms of Use\" section.\n",
    "- Click the button to accept and login with your user if you haven't already. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try an example request query\n",
    "\n",
    "Earthkit provides a convenience method for retrieving data from CDS, `earthkit.data.from_source(\"cds\", ...)`. To obtain the correct parameters to use for the data query, you can follow these steps:\n",
    "\n",
    "- Manually go to the dataset [Download page](https://cds.climate.copernicus.eu/datasets/derived-era5-single-levels-daily-statistics?tab=download)\n",
    "  - In our case, select 2m Temperature, select all days of a single month, and a subregion containing Sierra Leone. \n",
    "- At the bottom of the page, click \"Show API Request Code\" in the \"API Request\" section.\n",
    "- This should show something like this:\n",
    "\n",
    "        import cdsapi\n",
    "\n",
    "        dataset = \"derived-era5-single-levels-daily-statistics\"\n",
    "        request = {\n",
    "            \"product_type\": \"reanalysis\",\n",
    "            \"variable\": [\"2m_temperature\"],\n",
    "            \"year\": \"2024\",\n",
    "            \"month\": [\"12\"],\n",
    "            \"day\": [\n",
    "                \"01\", \"02\", \"03\",\n",
    "                \"04\", \"05\", \"06\",\n",
    "                \"07\", \"08\", \"09\",\n",
    "                \"10\", \"11\", \"12\",\n",
    "                \"13\", \"14\", \"15\",\n",
    "                \"16\", \"17\", \"18\",\n",
    "                \"19\", \"20\", \"21\",\n",
    "                \"22\", \"23\", \"24\",\n",
    "                \"25\", \"26\", \"27\",\n",
    "                \"28\", \"29\", \"30\",\n",
    "                \"31\"\n",
    "            ],\n",
    "            \"daily_statistic\": \"daily_mean\",\n",
    "            \"time_zone\": \"utc+00:00\",\n",
    "            \"frequency\": \"1_hourly\",\n",
    "            \"area\": [10.0, -13.3, 6.9, -10.3],\n",
    "        }\n",
    "\n",
    "        client = cdsapi.Client()\n",
    "        client.retrieve(dataset, request).download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `earthkit` uses `cdsapi` in the background, we can copy these parameter values directly to fill in the parameters required by `earthkit`. We also add two additional parameters to download as unzipped NetCDF format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 22:42:30,964 INFO Request ID is 031b6ad4-71d0-4c11-aa07-f37d7afe839c\n",
      "2025-10-08 22:42:31,270 INFO status has been updated to accepted\n",
      "2025-10-08 22:42:39,739 INFO status has been updated to running\n",
      "2025-10-08 22:43:21,398 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133cf2b7d8a04f81843ff274972923d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "c8eb0315aded24d457343fa69159ad9d.nc:   0%|          | 0.00/34.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = earthkit.data.from_source(\"cds\",\n",
    "    \"derived-era5-single-levels-daily-statistics\",\n",
    "    {\n",
    "        \"product_type\": \"reanalysis\",\n",
    "        \"variable\": [\"2m_temperature\"],\n",
    "        \"year\": \"2024\",\n",
    "        \"month\": [\"12\"],\n",
    "        \"day\": [\n",
    "            \"01\", \"02\", \"03\",\n",
    "            \"04\", \"05\", \"06\",\n",
    "            \"07\", \"08\", \"09\",\n",
    "            \"10\", \"11\", \"12\",\n",
    "            \"13\", \"14\", \"15\",\n",
    "            \"16\", \"17\", \"18\",\n",
    "            \"19\", \"20\", \"21\",\n",
    "            \"22\", \"23\", \"24\",\n",
    "            \"25\", \"26\", \"27\",\n",
    "            \"28\", \"29\", \"30\",\n",
    "            \"31\"\n",
    "        ],\n",
    "        \"daily_statistic\": \"daily_mean\",\n",
    "        \"time_zone\": \"utc+00:00\",\n",
    "        \"frequency\": \"1_hourly\",\n",
    "        \"area\": [10.0, -13.3, 6.9, -10.3],\n",
    "        \"data_format\": \"netcdf\",\n",
    "        \"download_format\": \"unarchived\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a generalized function for downloading data\n",
    "\n",
    "In order to make this more useful, we generalize this in a function that modifies some of the download parameters based on input arguments as described below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Year and month inputs\n",
    "\n",
    "In our function we want the user to be able to input a `year` and `month`, and then update the necessary query parameters. To automatically select all individual days for a particular month, we can use the builtin Python function `calendar.monthrange(year, month)` to get the first and last day of each month. We also have to left-pad all numbers with zero. This function might look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_temperature_for_month(year, month):\n",
    "    import calendar\n",
    "    # construct the query parameters\n",
    "    params = {\n",
    "        \"product_type\": \"reanalysis\",\n",
    "        \"variable\": [\"2m_temperature\"],\n",
    "        \"year\": str(year),\n",
    "        \"month\": [str(month).zfill(2)],\n",
    "        \"daily_statistic\": \"daily_mean\",\n",
    "        \"time_zone\": \"utc+00:00\",\n",
    "        \"frequency\": \"1_hourly\",\n",
    "        \"area\": [10.0, -13.3, 6.9, -10.3],\n",
    "        \"data_format\": \"netcdf\",\n",
    "        \"download_format\": \"unarchived\",\n",
    "    }\n",
    "    first_day,last_day = calendar.monthrange(year, month)\n",
    "    params['day'] = [str(day).zfill(2) for day in range(first_day, last_day)]\n",
    "    print(params)\n",
    "    # download the data\n",
    "    data = earthkit.data.from_source(\"cds\",\n",
    "        \"derived-era5-single-levels-daily-statistics\",\n",
    "        params,\n",
    "    )\n",
    "    # return\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining the area coordinates from organisation units\n",
    "\n",
    "Notice that we didn't yet do anything with the `area` parameter. To make our function more generic we also want to set this `area` parameter based on the bounding box of our organisation unit geometries. We therefore add another function that calculates the bounding box from a `geopandas.GeoDataFrame` and allow a bounding box input to our download function. The updated code would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "def get_bbox(org_units: gpd.GeoDataFrame):\n",
    "    '''Returns bounding box of a geopandas GeoDataFrame in standard format: xmin,ymin,xmax,ymax.'''\n",
    "    bbox = org_units.total_bounds\n",
    "    return bbox\n",
    "\n",
    "def download_temperature_for_month_and_bbox(year, month, bbox):\n",
    "    import calendar\n",
    "    # extract the coordinates from input bounding box\n",
    "    xmin,ymin,xmax,ymax = bbox\n",
    "    # construct the query parameters\n",
    "    params = {\n",
    "        \"product_type\": \"reanalysis\",\n",
    "        \"variable\": [\"2m_temperature\"],\n",
    "        \"year\": str(year),\n",
    "        \"month\": [str(month).zfill(2)],\n",
    "        \"daily_statistic\": \"daily_mean\",\n",
    "        \"time_zone\": \"utc+00:00\",\n",
    "        \"frequency\": \"1_hourly\",\n",
    "        \"area\": [ymax, xmin, ymin, xmax], # notice how we reordered the bbox coordinate sequence\n",
    "        \"data_format\": \"netcdf\",\n",
    "        \"download_format\": \"unarchived\",\n",
    "    }\n",
    "    first_day,last_day = calendar.monthrange(year, month)\n",
    "    params['day'] = [str(day).zfill(2) for day in range(first_day, last_day)]\n",
    "    # download the data\n",
    "    data = earthkit.data.from_source(\"cds\",\n",
    "        \"derived-era5-single-levels-daily-statistics\",\n",
    "        params,\n",
    "    )\n",
    "    # return\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caching download results\n",
    "\n",
    "Since data downloads can be slow, we also want to cache the download results and reuse if the file has already been downloaded. We wrap our previous download function, save the results of each download into a local folder, or load the data from disk if the file has already been downloaded. Since we are working with a daily updated dataset, we make sure we only cache downloads for completed months. The cached wrapper function for getting temperature data looks like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temperature_data(year, month, bbox, cache_folder='../data/local'):\n",
    "    import os\n",
    "    from datetime import date\n",
    "    current_date = date.today()\n",
    "    # convert input args to a cache filename\n",
    "    xmin,ymin,xmax,ymax = bbox\n",
    "    file_name = f'temperature_{year}-{str(month).zfill(2)}_bbox_{int(xmin)}_{int(ymin)}_{int(xmax)}_{int(ymax)}.nc'\n",
    "    file_path = os.path.join(cache_folder, file_name)\n",
    "    # check if cache filename already exists\n",
    "    if os.path.exists(file_path):\n",
    "        # load from cache\n",
    "        print('Loading from cache', file_path)\n",
    "        data = earthkit.data.from_source('file', file_path)\n",
    "    else:\n",
    "        # download data from the api\n",
    "        print('Downloading from api...')\n",
    "        data = download_temperature_for_month_and_bbox(year, month, bbox)\n",
    "        # save to cache, but not if we're still in the current month\n",
    "        if year == current_date.year and month == current_date.month:\n",
    "            print('Data is for the current month and will not be cached, since data is added daily')\n",
    "        else:\n",
    "            print('Saving to cache', file_path)\n",
    "            data.to_target('file', file_path)            \n",
    "    # return\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test case\n",
    "\n",
    "Finally, let's try our final download function for February of 2012 and a set of organisation units from Sierra Leone: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bbox: [-13.3035   6.9176 -10.2658  10.0004]\n",
      "Loading from cache ../data/local\\temperature_2012-02_bbox_-13_6_-10_10.nc\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# get org units\n",
    "org_unit_level = 2\n",
    "geojson = client.get_org_units_geojson(level=org_unit_level)\n",
    "\n",
    "# add org unit id to properties\n",
    "for feat in geojson['features']:\n",
    "    feat['properties']['org_unit_id'] = feat['id']\n",
    "\n",
    "# convert to geopandas\n",
    "org_units = gpd.GeoDataFrame.from_features(geojson[\"features\"])\n",
    "\n",
    "# calc bbox\n",
    "bbox = get_bbox(org_units)\n",
    "print('Bbox:', bbox)\n",
    "\n",
    "# download data\n",
    "data = get_temperature_data(2012, 2, bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------\n",
    "\n",
    "## Aggregating the data to organisation units\n",
    "\n",
    "The next step is creating a generic function that aggregates the data downloaded from the previous step to a set of input organisation units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(data, org_units, id_col):\n",
    "    from earthkit.transforms import aggregate\n",
    "    # aggregate to org unit for each time period\n",
    "    agg_data = aggregate.spatial.reduce(data, org_units, mask_dim=id_col)\n",
    "    # convert to dataframe\n",
    "    agg_df = agg_data.to_dataframe().reset_index()\n",
    "    # return\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it for our previously downloaded test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    valid_time  org_unit_id  number        t2m\n",
      "0   2012-02-02  O6uvpzGd5pu       0  27.335388\n",
      "1   2012-02-02  fdc6uOvgoji       0  27.853302\n",
      "2   2012-02-02  lc3eMKXaEfw       0  26.812439\n",
      "3   2012-02-02  jUb8gELQApl       0  27.563202\n",
      "4   2012-02-02  PMa2VCrupOd       0  27.259949\n",
      "..         ...          ...     ...        ...\n",
      "346 2012-02-28  jmIPBj66vD6       0  26.785797\n",
      "347 2012-02-28  TEQlaapDQoK       0  27.028656\n",
      "348 2012-02-28  bL4ooGhyHRQ       0  26.023712\n",
      "349 2012-02-28  eIQbndfxQMb       0  27.649323\n",
      "350 2012-02-28  at6UHUQatSo       0        NaN\n",
      "\n",
      "[351 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "agg = aggregate(data, org_units, id_col='org_unit_id')\n",
    "agg['t2m'] -= 273.15 # convert to celsius\n",
    "print(agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the aggregated data contains temperature values for each organisation unit (`org_unit_id`) and all the 28 days in February 2012 contained in the downloaded NetCDF data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------\n",
    "\n",
    "## Determining the data period for importing\n",
    "\n",
    "Now that we have a simple way to download and aggregate temperature data, we want a function that defines a time period for which we want data. We have two goals here:\n",
    "\n",
    "1. Since data is downloaded and processed on a monthly basis we want to return which year-month period we want to process. \n",
    "2. Return all year-month periods between the last valid data value for a given data element and today's date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_month_periods_since_last_data_value(data_element_id, earliest_year, earliest_month):\n",
    "    from datetime import date\n",
    "    # get current year and month\n",
    "    current_date = date.today()\n",
    "    current_year,current_month = current_date.year, current_date.month\n",
    "    # get last year and month for which data values exist in dhis2 data element\n",
    "    first_period_response = {'existing': None} # TODO: update once daily periods are supported # client.analytics_latest_period_for_level(de_uid=data_element_id, level=org_unit_level)\n",
    "    if first_period_response['existing']: \n",
    "        # last data value found\n",
    "        first_period = first_period_response['existing']['id']\n",
    "        first_year,first_month = int(first_period[:4]), int(first_period[4:6])\n",
    "        # but no earlier than earliest year-month\n",
    "        first_year = max(earliest_year, first_year)\n",
    "        first_month = max(earliest_month, first_month)\n",
    "    else:\n",
    "        # no data values exists, start at earliest year-month\n",
    "        first_year,first_month = earliest_year,earliest_month\n",
    "    # loop years and months between last dhis2 value and today's date\n",
    "    for year in range(first_year, current_year + 1):\n",
    "        start_month = first_month if year == first_year else 1\n",
    "        end_month = current_month if year == current_year else 12\n",
    "        for month in range(start_month, end_month + 1):\n",
    "            # yield year-month pairs\n",
    "            yield year,month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "## The full workflow\n",
    "\n",
    "Now we have all the components needed to automatically download data from the Climate Data Store. In this last section we will tie all the pieces together into a single function, which we can use to easily perform the data import at regular intervals: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_element_id, earliest_year, earliest_month):\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    from dhis2eo.integrations.pandas import dataframe_to_dhis2_json\n",
    "\n",
    "    # download org units geojson\n",
    "    print('Getting organisation units...')\n",
    "    geojson = client.get_org_units_geojson(level=org_unit_level)\n",
    "\n",
    "    # add org unit id to properties\n",
    "    for feat in geojson['features']:\n",
    "        feat['properties']['org_unit_id'] = feat['id']\n",
    "\n",
    "    # convert to geopandas and get bbox\n",
    "    org_units = gpd.GeoDataFrame.from_features(geojson[\"features\"])\n",
    "    bbox = get_bbox(org_units)\n",
    "\n",
    "    # fetch, aggregate, and import data month-by-month\n",
    "    for year, month in iter_month_periods_since_last_data_value(data_element_id, earliest_year, earliest_month):\n",
    "        print('====================')\n",
    "        print('Period:', year, month)\n",
    "        # download data\n",
    "        print('Getting data...')\n",
    "        data = get_temperature_data(year, month, bbox)\n",
    "        # aggregate to org units\n",
    "        print('Aggregating...')\n",
    "        agg = aggregate(data, org_units, id_col='org_unit_id')\n",
    "        # convert to celsius\n",
    "        agg['t2m'] -= 273.15\n",
    "        # ignore nan values\n",
    "        agg = agg[~pd.isna(agg['t2m'])]\n",
    "        # convert to dhis2 json\n",
    "        payload = dataframe_to_dhis2_json(\n",
    "            df=agg,\n",
    "            org_unit_col='org_unit_id',\n",
    "            period_col='valid_time',\n",
    "            value_col='t2m',\n",
    "            data_element_id=data_element_id,\n",
    "        )\n",
    "        # upload to dhis2\n",
    "        print('Importing to DHIS2...')\n",
    "        res = client.post(\"/api/dataValueSets\", json=payload)\n",
    "        print(\"Results:\", res['response']['importCount'])\n",
    "\n",
    "    print('=====================')\n",
    "    print('Data import finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try to run the function to import daily temperature data since 1 January 2025 until today: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting organisation units...\n",
      "====================\n",
      "Period: 2025 1\n",
      "Getting data...\n",
      "Loading from cache ../data/local\\temperature_2025-01_bbox_-13_6_-10_10.nc\n",
      "Aggregating...\n",
      "Importing to DHIS2...\n",
      "Results: {'imported': 0, 'updated': 348, 'ignored': 0, 'deleted': 0}\n",
      "====================\n",
      "Period: 2025 2\n",
      "Getting data...\n",
      "Loading from cache ../data/local\\temperature_2025-02_bbox_-13_6_-10_10.nc\n",
      "Aggregating...\n",
      "Importing to DHIS2...\n",
      "Results: {'imported': 0, 'updated': 276, 'ignored': 0, 'deleted': 0}\n",
      "====================\n",
      "Period: 2025 3\n",
      "Getting data...\n",
      "Loading from cache ../data/local\\temperature_2025-03_bbox_-13_6_-10_10.nc\n",
      "Aggregating...\n",
      "Importing to DHIS2...\n",
      "Results: {'imported': 0, 'updated': 312, 'ignored': 0, 'deleted': 0}\n",
      "====================\n",
      "Period: 2025 4\n",
      "Getting data...\n",
      "Loading from cache ../data/local\\temperature_2025-04_bbox_-13_6_-10_10.nc\n",
      "Aggregating...\n",
      "Importing to DHIS2...\n",
      "Results: {'imported': 0, 'updated': 348, 'ignored': 0, 'deleted': 0}\n",
      "====================\n",
      "Period: 2025 5\n",
      "Getting data...\n",
      "Loading from cache ../data/local\\temperature_2025-05_bbox_-13_6_-10_10.nc\n",
      "Aggregating...\n",
      "Importing to DHIS2...\n",
      "Results: {'imported': 0, 'updated': 336, 'ignored': 0, 'deleted': 0}\n",
      "====================\n",
      "Period: 2025 6\n",
      "Getting data...\n",
      "Loading from cache ../data/local\\temperature_2025-06_bbox_-13_6_-10_10.nc\n",
      "Aggregating...\n",
      "Importing to DHIS2...\n",
      "Results: {'imported': 0, 'updated': 288, 'ignored': 0, 'deleted': 0}\n",
      "====================\n",
      "Period: 2025 7\n",
      "Getting data...\n",
      "Loading from cache ../data/local\\temperature_2025-07_bbox_-13_6_-10_10.nc\n",
      "Aggregating...\n",
      "Importing to DHIS2...\n",
      "Results: {'imported': 0, 'updated': 360, 'ignored': 0, 'deleted': 0}\n",
      "====================\n",
      "Period: 2025 8\n",
      "Getting data...\n",
      "Loading from cache ../data/local\\temperature_2025-08_bbox_-13_6_-10_10.nc\n",
      "Aggregating...\n",
      "Importing to DHIS2...\n",
      "Results: {'imported': 0, 'updated': 324, 'ignored': 0, 'deleted': 0}\n",
      "====================\n",
      "Period: 2025 9\n",
      "Getting data...\n",
      "Loading from cache ../data/local\\temperature_2025-09_bbox_-13_6_-10_10.nc\n",
      "Aggregating...\n",
      "Importing to DHIS2...\n",
      "Results: {'imported': 0, 'updated': 348, 'ignored': 0, 'deleted': 0}\n",
      "====================\n",
      "Period: 2025 10\n",
      "Getting data...\n",
      "Downloading from api...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 08:59:01,894 INFO Request ID is c8242a40-9ded-41e2-ab18-ee811e8edb0f\n",
      "2025-10-09 08:59:01,982 INFO status has been updated to accepted\n",
      "2025-10-09 08:59:10,703 INFO status has been updated to running\n",
      "2025-10-09 08:59:23,579 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ede7f7f6184a99a882aa39a194a8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "7e36e70de28fdff87153c77cd2a5d634.nc:   0%|          | 0.00/22.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is for the current month and will not be cached, since data is added daily\n",
      "Aggregating...\n",
      "Importing to DHIS2...\n",
      "Results: {'imported': 0, 'updated': 24, 'ignored': 0, 'deleted': 0}\n",
      "=====================\n",
      "Data import finished!\n"
     ]
    }
   ],
   "source": [
    "data_element_id = 'GbUpvHzCzn8' # data element id that you want to import data into\n",
    "start_year = 2025\n",
    "start_month = 1\n",
    "main(data_element_id, start_year, start_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Running this data import function multiple times in the same month, will result in the entire month being downloaded and imported each time, since the data is updated on a daily basis. But the results from the data import will report how many data values already existed and were ignored, and how many new data values were imported since last time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "In this notebook we have created a function that can be run at regular intervals, e.g. every day or week, to fetch and import only the latest temperature data for your org units. But we still need a way to run the script. This can be done either manually, or automatically via a `cron` job. Further guidance on how to automatically schedule running a script will be added in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate-tools-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
