{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e74b011a",
   "metadata": {},
   "source": [
    "---\n",
    "title: Importing data using dhis2-python-client\n",
    "short_title: data import with python client\n",
    "---\n",
    "\n",
    "This section walks you through importing data values into DHIS2 using the **`dhis2-python-client`** library.\n",
    "\n",
    "**Below are crucial steps to follow:**\n",
    "- Get credentials\n",
    "- Connect to DHIS2 and verify access\n",
    "- Build a valid `dataValueSets` payload (JSON or CSV)\n",
    "- Send your payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a14f52",
   "metadata": {},
   "source": [
    "## 1) Configure your environment and connect to DHIS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de00fa95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Current DHIS2 version: 2.42.2-SNAPSHOT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dhis2_client import DHIS2Client\n",
    "from dhis2_client.settings import ClientSettings\n",
    "\n",
    "# Client configuration\n",
    "cfg = ClientSettings(\n",
    "  base_url=\"http://localhost:8080\",\n",
    "  username=\"admin\",\n",
    "  password=\"district\")\n",
    "\n",
    "client = DHIS2Client(settings=cfg)\n",
    "info = client.get_system_info()\n",
    "\n",
    "# Check if everything is working.\n",
    "# You should see your current DHIS2 version info.\n",
    "print(\"▶ Current DHIS2 version:\", info[\"version\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f7f90a",
   "metadata": {},
   "source": [
    "## 2) Prepare metadata\n",
    "\n",
    "There are two paths to prepare your payload. The first and probably easier path is to use existing metadata, i.e. organisation units, data elements, data sets, period and related category and attribute option combos if available. The second one is to create all of these from scratch and `dhis2-python-client` allows you to do this easily. For exmaple below is a simple step to create a organisation unit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d7de4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Organizational unit create status: OK and UID: at4fZqDz0lW\n"
     ]
    }
   ],
   "source": [
    "ou = {\"name\": \"My organisation unit\", \"shortName\": \"My org unit\", \"openingDate\": \"2020-01-01\"}\n",
    "org_unit_response = client.create_org_unit(ou)\n",
    "print(f\"▶ Organizational unit create status: {org_unit_response['status']} and UID: {org_unit_response['response']['uid']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcb657f",
   "metadata": {},
   "source": [
    "Since we haven't assigned a parent to the above org unit, it will be created at the root level. For subsquent org units, we could assign `parent': {'id': 'at4fZqDz0lW'}` and they would be placed underneath `My organisation unit`. This way we could create the entire org unit tree. Once org units are created, we can proceed to creating data elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7cab75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Data element create status: OK and UID: i9x1eWzeZzS\n"
     ]
    }
   ],
   "source": [
    "data_element = {\n",
    "    \"name\": \"My Sample data element\",\n",
    "    \"shortName\": \"Sample data element\",\n",
    "    \"valueType\": \"NUMBER\",\n",
    "    \"aggregationType\": \"SUM\",\n",
    "    \"domainType\": \"AGGREGATE\"\n",
    "}\n",
    "data_element_response = client.create_data_element(data_element)\n",
    "print(f\"▶ Data element create status: {data_element_response['status']} and UID: {data_element_response['response']['uid']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f730f720",
   "metadata": {},
   "source": [
    "Next in the metadata creation process is `data sets`. Data sets are crucial in defining how often we will be collecting value for a data element, i.e. they help us define `periods`. Not only that, data sets also help us connect a data element with an org unit. Below is how to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f9ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Data set create status: OK and UID: lFKYnmcAse0\n"
     ]
    }
   ],
   "source": [
    "data_set = {\n",
    "    \"name\": \"My sample data set\", \n",
    "    \"shortName\": \"Sample data set\",\n",
    "    \"periodType\": \"Monthly\",\n",
    "    \"dataSetElements\": [\n",
    "        {\n",
    "            \"dataElement\": {\"id\": data_element_response['response']['uid']}\n",
    "        }\n",
    "    ],\n",
    "    \"organisationUnits\": [\n",
    "        {\n",
    "            \"id\": org_unit_response['response']['uid']\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "data_set_response = client.create_data_set(data_set)\n",
    "print(f\"▶ Data set create status: {data_set_response['status']} and UID: {data_set_response['response']['uid']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbad190",
   "metadata": {},
   "source": [
    "Once data set is created, next step is to grant dat awrite access. The following can give current user data write access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9bb84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharing = client.grant_self_data_write_on_dataset(data_set_response['response']['uid'])\n",
    "\n",
    "print(\"✔ Granted myself data write on the dataset\", sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeca41a",
   "metadata": {},
   "source": [
    "At this point you should have your metadata ready to accept data. Next step is to construct data payload and do a POST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba170d6",
   "metadata": {},
   "source": [
    "## 3) Construct payload\n",
    "\n",
    "Data values can be sent to DHIS2 one-by-one or in batch. The one-by-one approach is very handy espeically when users are entering data manually. For batch import DHIS2 provides `dataValueSets` endpint. Below is a sample payload demonstrating batch data import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155aacf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"dataValues\": [\n",
    "        {\n",
    "            \"dataElement\": data_element_response['response']['uid'],\n",
    "            \"orgUnit\": org_unit_response['response']['uid'],\n",
    "            \"period\": \"202501\",\n",
    "            \"value\": 2501\n",
    "        },\n",
    "        {\n",
    "            \"dataElement\": data_element_response['response']['uid'],\n",
    "            \"orgUnit\": org_unit_response['response']['uid'],\n",
    "            \"period\": \"202502\",\n",
    "            \"value\": 2502\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a38ae5",
   "metadata": {},
   "source": [
    "## 4) Send your payload\n",
    "\n",
    "Once we are done preparing our payload we can proceed to using send it to DHIS2. `dhis2-python-client` provides both direct access to raw DHIS2 API like `client.post(/api/dataValueSets, ...)` or convenient method `client.post_data_value_set(...)`. Since we saw the convenient methods in the above metadata creation steps let's use the raw DHIS2 API this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d411ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Data value set post status:  {'httpStatus': 'OK', 'httpStatusCode': 200, 'status': 'OK', 'message': 'Import was successful.', 'response': {'status': 'SUCCESS', 'importOptions': {'idSchemes': {}, 'dryRun': False, 'async': False, 'importStrategy': 'CREATE_AND_UPDATE', 'mergeMode': 'REPLACE', 'reportMode': 'FULL', 'skipExistingCheck': False, 'sharing': False, 'skipNotifications': False, 'skipAudit': False, 'datasetAllowsPeriods': False, 'strictPeriods': False, 'strictDataElements': False, 'strictCategoryOptionCombos': False, 'strictAttributeOptionCombos': False, 'strictOrganisationUnits': False, 'strictDataSetApproval': False, 'strictDataSetLocking': False, 'strictDataSetInputPeriods': False, 'requireCategoryOptionCombo': False, 'requireAttributeOptionCombo': False, 'skipPatternValidation': False, 'ignoreEmptyCollection': False, 'force': False, 'firstRowIsHeader': True, 'skipLastUpdated': False, 'mergeDataValues': False, 'skipCache': False}, 'description': 'Import process completed successfully', 'importCount': {'imported': 2, 'updated': 0, 'ignored': 0, 'deleted': 0}, 'conflicts': [], 'rejectedIndexes': [], 'dataSetComplete': 'false', 'responseType': 'ImportSummary'}}\n"
     ]
    }
   ],
   "source": [
    "res = client.post(\"/api/dataValueSets\", json=payload)\n",
    "print(\"▶ Data value set post status: \", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de68d8",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Troubleshooting\n",
    "\n",
    "- **Unauthorized**: Check credentials and user permissions.  \n",
    "- **Not found**: Verify data element, org unit, and combos (if you have used non-default ones) exist.  \n",
    "- **Conflicts**: Ensure dataset assignments and period are correct.  \n",
    "- **Locked periods**: Unlock dataset period if needed.  \n",
    "- **Value types**: Match the data element value type.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
