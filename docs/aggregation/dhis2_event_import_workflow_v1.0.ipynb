{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beec85b2",
   "metadata": {},
   "source": [
    "# Importing mobidity data to dhis2 event program\n",
    "**Workflow:** load monthly CSVs â†’ filter ICD10 J-codes â†’ map to Event program â†’ batch import\n",
    "\n",
    "> Tip: Run cells top-to-bottom. Update the config in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709fe65e-7514-466d-abba-332433b7272c",
   "metadata": {},
   "source": [
    "### Step 1: Defining Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6255e88-2940-4155-9d41-bfc09c954285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded (CSV mode). OrgUnit map entries: 28.\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 1 â€” Defining Variables (CSV version) =====\n",
    "\n",
    "# --- File layout (CSV in yearly folders) ---\n",
    "# Example:\n",
    "#   ...\\immr data\\\n",
    "#       eIMMR raw data 2019\\Jan 2019.csv\n",
    "#       eIMMR raw data 2019\\Feb 2019.csv\n",
    "#       eIMMR raw data 2020\\Jan 2020.csv\n",
    "ROOT_DATA_DIR = r\"...\\immr data\" # ðŸ”¹ Give the correct path\n",
    "INCLUDE_EXTENSIONS = {\".csv\"}\n",
    "\n",
    "# If headers are not on the first row, adjust these\n",
    "HEADER_ROW = 0            # 0-based index of header row\n",
    "SKIPROWS = []             # e.g., [0,1] to skip top lines before header\n",
    "\n",
    "# Import ONLY these columns (+ region_name for orgUnit mapping)\n",
    "USECOLS = [\n",
    "    \"region_name\",      # used to map to DHIS2 orgUnit\n",
    "    \"age\",\n",
    "    \"sex\",\n",
    "    \"discharge_mode\",\n",
    "    \"admission_date\",\n",
    "    \"discharge_date\",\n",
    "    \"days\",\n",
    "    \"icd\",\n",
    "]\n",
    "\n",
    "# --- Column semantics for later steps ---\n",
    "COLS = {\n",
    "    \"orgunit_source\": \"region_name\",\n",
    "    \"event_date\": \"admission_date\",   # change to \"discharge_date\" if preferred\n",
    "    \"icd\": \"icd\",\n",
    "    \"sex\": \"sex\",\n",
    "    \"age_years\": \"age\",\n",
    "    \"discharge_mode\": \"discharge_mode\",\n",
    "    \"length_of_stay_days\": \"days\",\n",
    "    \"discharge_date\": \"discharge_date\",\n",
    "}\n",
    "\n",
    "# ===== Org unit mapping (CSV) =====\n",
    "# CSV must have columns: region_name, orgUnitId\n",
    "ORGUNIT_MAP_CSV = r\"....\\immr data\\orgunit_mapping_region_to_uid.csv\" # ðŸ”¹ Give the correct path\n",
    "STRICT_ORGUNIT_MATCH = True\n",
    "\n",
    "# --- DHIS2 target (Event program) ---\n",
    "DHIS2_BASE_URL = \"<URL of the instance>\"  \n",
    "DHIS2_USERNAME = \"<UN>\"\n",
    "DHIS2_PASSWORD = \"<PW>\"  # basic auth (Phase 1)\n",
    "\n",
    "PROGRAM_ID = \"<Program UID>\"\n",
    "PROGRAM_STAGE_ID = \"<Program stage UID>\"\n",
    "\n",
    "# Map columns â†’ DHIS2 dataElement UIDs (belonging to PROGRAM_STAGE_ID)\n",
    "DE_MAP = {\n",
    "    \"sex\": \"add_UID\",                   # ðŸ”¹ add correct UID\n",
    "    \"age_years\": \"add_UID\",             # ðŸ”¹ add correct UID\n",
    "    \"discharge_mode\": \"add_UID\",        # ðŸ”¹ add correct UID\n",
    "    \"length_of_stay_days\": \"add_UID\",   # ðŸ”¹ add correct UID\n",
    "    \"icd\": \"add_UID\",                   # ðŸ”¹ add correct UID\n",
    "    \"discharge_date\": \"add_UID\"         # ðŸ”¹ add correct UID\n",
    "}\n",
    "\n",
    "# DHIS2 event fields\n",
    "DEFAULT_EVENT_STATUS = \"COMPLETED\"\n",
    "STORED_BY = \"climate-tools-notebook\"\n",
    "\n",
    "# --- Import behavior ---\n",
    "BATCH_SIZE = 500\n",
    "DRY_RUN_FIRST = True\n",
    "VERIFY_SSL = True\n",
    "\n",
    "# --- Helper: case-insensitive orgUnit resolver from region_name (using CSV) ---\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def _normalize_key(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(s).strip()).lower()\n",
    "\n",
    "def load_orgunit_map(csv_path: str) -> dict:\n",
    "    df = pd.read_csv(csv_path, dtype=str)\n",
    "    if not {\"region_name\", \"orgUnitId\"}.issubset(df.columns):\n",
    "        raise ValueError(\"ORGUNIT_MAP_CSV must have columns: region_name, orgUnitId\")\n",
    "    df[\"region_name_norm\"] = df[\"region_name\"].map(_normalize_key)\n",
    "    df[\"orgUnitId\"] = df[\"orgUnitId\"].str.strip()\n",
    "    df = df.dropna(subset=[\"region_name_norm\", \"orgUnitId\"])\n",
    "    return dict(zip(df[\"region_name_norm\"], df[\"orgUnitId\"]))\n",
    "\n",
    "_orgunit_map = load_orgunit_map(ORGUNIT_MAP_CSV)\n",
    "\n",
    "def resolve_orgunit_from_region(region_value: str):\n",
    "    key = _normalize_key(region_value)\n",
    "    if key in _orgunit_map:\n",
    "        return _orgunit_map[key]\n",
    "    if STRICT_ORGUNIT_MATCH:\n",
    "        raise KeyError(f\"Unknown region_name for orgUnit mapping: '{region_value}'\")\n",
    "    return region_value  # pass-through if non-strict\n",
    "\n",
    "print(f\"Config loaded (CSV mode). OrgUnit map entries: {len(_orgunit_map)}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad7bbd2",
   "metadata": {},
   "source": [
    "### Step 2: Setting up environment & libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a4f65-a191-457b-8cb8-393f2fd093ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 2 â€” Environment & Libraries (CSV mode) =====\n",
    "\n",
    "# If needed, uncomment to install:\n",
    "# %pip install -q pandas python-dateutil requests\n",
    "\n",
    "import os, sys, glob, re, json, math, warnings\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse as dtparse\n",
    "import requests\n",
    "\n",
    "def _ver(mod):\n",
    "    try:\n",
    "        return mod.__version__\n",
    "    except Exception:\n",
    "        return \"n/a\"\n",
    "\n",
    "print(\"Environment versions:\")\n",
    "print(\"  Python   :\", sys.version.split()[0])\n",
    "print(\"  pandas   :\", _ver(pd))\n",
    "print(\"  requests :\", _ver(requests))\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Reusable HTTP session\n",
    "session = requests.Session()\n",
    "session.verify = VERIFY_SSL\n",
    "session.auth = (DHIS2_USERNAME, DHIS2_PASSWORD)\n",
    "session.headers.update({\"Content-Type\": \"application/json\"})\n",
    "\n",
    "def dhis2_ping():\n",
    "    \"\"\"Quick connectivity check to DHIS2 system info (optional).\"\"\"\n",
    "    try:\n",
    "        r = session.get(f\"{DHIS2_BASE_URL}/api/system/info\", timeout=20)\n",
    "        print(\"Ping status:\", r.status_code)\n",
    "        if r.ok:\n",
    "            info = r.json()\n",
    "            print(\"System name  :\", info.get(\"systemName\"))\n",
    "            print(\"Version      :\", info.get(\"version\"))\n",
    "            print(\"Context path :\", info.get(\"contextPath\"))\n",
    "        else:\n",
    "            print(\"Response text:\", r.text[:300])\n",
    "    except Exception as e:\n",
    "        print(\"Ping failed:\", e)\n",
    "\n",
    "print(\"Modules loaded. Ready for CSV ingestion.\")\n",
    "print(\"Checking the connection to dhis2...\")\n",
    "def dhis2_ping():\n",
    "    \"\"\"Quick connectivity check to DHIS2 system info (optional).\"\"\"\n",
    "    try:\n",
    "        r = session.get(f\"{DHIS2_BASE_URL}/api/system/info\", timeout=20)\n",
    "        print(\"Ping status:\", r.status_code)\n",
    "        if r.ok:\n",
    "            info = r.json()\n",
    "            print(\"System name  :\", info.get(\"systemName\"))\n",
    "            print(\"Version      :\", info.get(\"version\"))\n",
    "            print(\"Context path :\", info.get(\"contextPath\"))\n",
    "        else:\n",
    "            print(\"Response text:\", r.text[:300])\n",
    "    except Exception as e:\n",
    "        print(\"Ping failed:\", e)\n",
    "dhis2_ping()\n",
    "print(\"Task completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d686e4",
   "metadata": {},
   "source": [
    "### Step 3: Load monthly CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ec9bda-656d-4d74-8ab6-0bc2c43c9947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 3 â€” Load monthly CSV files (robust; skip mapping; derive `days`) =====\n",
    "from pathlib import Path\n",
    "import pandas as pd, csv, re\n",
    "\n",
    "# --- Only include data files inside year folders like \"eIMMR raw data 2019\", etc. ---\n",
    "DATA_DIR_PREFIX = \"eIMMR raw data\"  # only folders starting with this are considered data folders\n",
    "\n",
    "# --- Skip-list: filenames that must never be treated as data files ---\n",
    "SKIP_FILE_NAMES = {\"orgunit_mapping_region_to_uid.csv\"}  # mapping file\n",
    "\n",
    "# --- Header normalization + aliasing (case/space-insensitive) ---\n",
    "_norm = lambda s: re.sub(r\"\\s+\", \" \", str(s).strip()).lower()\n",
    "\n",
    "HEADER_ALIASES = {\n",
    "    \"region\": \"region_name\",\n",
    "    \"district\": \"region_name\",\n",
    "    \"district_name\": \"region_name\",\n",
    "    \"icd10\": \"icd\",\n",
    "    \"icd_code\": \"icd\",\n",
    "    \"length_of_stay\": \"days\",\n",
    "    \"los\": \"days\",\n",
    "    \"no_of_days\": \"days\",\n",
    "}\n",
    "\n",
    "# Treat `days` as optional (weâ€™ll derive it if missing)\n",
    "REQUIRED_BASE_COLS = [c for c in USECOLS if c != \"days\"]\n",
    "OPTIONAL_DERIVED_COLS = {\"days\"}\n",
    "\n",
    "def align_headers(df: pd.DataFrame, expected_cols: list[str]) -> pd.DataFrame:\n",
    "    cur = {_norm(c): c for c in df.columns}\n",
    "    rename_map = {}\n",
    "    for exp in expected_cols:\n",
    "        if exp in df.columns:\n",
    "            continue\n",
    "        exp_norm = _norm(exp)\n",
    "        # canonical or any alias that maps to this canonical name\n",
    "        alias_norms = {k for k, v in HEADER_ALIASES.items() if _norm(v) == exp_norm}\n",
    "        candidates = {exp_norm} | alias_norms\n",
    "        for cand in candidates:\n",
    "            if cand in cur:\n",
    "                rename_map[cur[cand]] = exp\n",
    "                break\n",
    "    if rename_map:\n",
    "        df = df.rename(columns=rename_map)\n",
    "    return df\n",
    "\n",
    "def sniff_delimiter_text(sample_text: str, default=\",\"):\n",
    "    try:\n",
    "        return csv.Sniffer().sniff(sample_text, delimiters=\",;\\t|\").delimiter\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def read_csv_guess(path: Path, usecols=None, header=HEADER_ROW, skiprows=SKIPROWS):\n",
    "    last_err = None\n",
    "    for enc in (\"utf-8-sig\", \"utf-8\", \"latin-1\"):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=enc, errors=\"ignore\") as fh:\n",
    "                sample = fh.read(4096)\n",
    "            sep = sniff_delimiter_text(sample, \",\")\n",
    "            return pd.read_csv(path, usecols=usecols, header=header, skiprows=skiprows, sep=sep, encoding=enc)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    return pd.read_csv(path, usecols=usecols, header=header, skiprows=skiprows,\n",
    "                       sep=\",\", encoding=\"latin-1\", on_bad_lines=\"skip\")\n",
    "\n",
    "def is_data_folder(p: Path) -> bool:\n",
    "    # include file only if any ancestor folder starts with \"eIMMR raw data\"\n",
    "    return any(part.startswith(DATA_DIR_PREFIX) for part in p.parts)\n",
    "\n",
    "def list_csv_files(root_dir: str) -> list[Path]:\n",
    "    p = Path(root_dir)\n",
    "    files = []\n",
    "    for f in p.rglob(\"*.csv\"):\n",
    "        if not f.is_file():\n",
    "            continue\n",
    "        if f.name.lower() in SKIP_FILE_NAMES:\n",
    "            continue\n",
    "        if not is_data_folder(f):\n",
    "            continue\n",
    "        files.append(f)\n",
    "    return sorted(files, key=lambda x: (x.parent.as_posix(), x.name))\n",
    "\n",
    "def read_month_file(path: Path) -> pd.DataFrame:\n",
    "    # try fast path with just the required base columns (+ optional days if present)\n",
    "    try:\n",
    "        fast_usecols = list({*REQUIRED_BASE_COLS, *OPTIONAL_DERIVED_COLS} & set(USECOLS))\n",
    "        df = read_csv_guess(path, usecols=fast_usecols, header=HEADER_ROW, skiprows=SKIPROWS)\n",
    "    except Exception:\n",
    "        # reload full, align headers, enforce only base required cols\n",
    "        df_all = read_csv_guess(path, usecols=None, header=HEADER_ROW, skiprows=SKIPROWS)\n",
    "        df_all = align_headers(df_all, REQUIRED_BASE_COLS + list(OPTIONAL_DERIVED_COLS))\n",
    "        missing_base = [c for c in REQUIRED_BASE_COLS if c not in df_all.columns]\n",
    "        if missing_base:\n",
    "            raise ValueError(f\"Missing required columns {missing_base}; found: {list(df_all.columns)}\")\n",
    "        keep = REQUIRED_BASE_COLS + [c for c in OPTIONAL_DERIVED_COLS if c in df_all.columns]\n",
    "        df = df_all[keep]\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # --- IMPORTANT: work on a real copy to avoid SettingWithCopyWarning ---\n",
    "    df = df.copy()\n",
    "\n",
    "    # Derive/patch `days`\n",
    "    have_days_col = \"days\" in df.columns\n",
    "    have_dates = {\"admission_date\", \"discharge_date\"}.issubset(df.columns)\n",
    "\n",
    "    if have_dates:\n",
    "        ad = pd.to_datetime(df[\"admission_date\"], errors=\"coerce\", infer_datetime_format=True)\n",
    "        dd = pd.to_datetime(df[\"discharge_date\"], errors=\"coerce\", infer_datetime_format=True)\n",
    "        days_calc = (dd - ad).dt.days\n",
    "\n",
    "        if have_days_col:\n",
    "            # only fill where days is missing/NaN\n",
    "            need_fill = df[\"days\"].isna()\n",
    "            df.loc[need_fill, \"days\"] = days_calc[need_fill]\n",
    "        else:\n",
    "            df.loc[:, \"days\"] = days_calc\n",
    "\n",
    "    # provenance (use .loc)\n",
    "    df.loc[:, \"__source_file\"] = path.name\n",
    "    df.loc[:, \"__source_year_folder\"] = path.parent.name\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---- Run loader ----\n",
    "files = list_csv_files(ROOT_DATA_DIR)\n",
    "print(f\"Found {len(files)} CSV files under data folders within: {ROOT_DATA_DIR}\")\n",
    "for f in files[:2]:\n",
    "    print(\"  -\", f)\n",
    "\n",
    "print(f\"\\nCombining rows from CSV. please wait...\")\n",
    "\n",
    "frames, errors = [], []\n",
    "for f in files:\n",
    "    try:\n",
    "        df = read_month_file(f)\n",
    "        if df is not None and len(df) > 0:\n",
    "            frames.append(df)\n",
    "        else:\n",
    "            errors.append((f, \"empty or no rows\"))\n",
    "    except Exception as e:\n",
    "        errors.append((f, str(e)))\n",
    "\n",
    "if not frames:\n",
    "    msg = \"No usable CSV files were readâ€”check ROOT_DATA_DIR, delimiters, and headers.\\n\"\n",
    "    for p, m in errors[:5]:\n",
    "        msg += f\"  - {p}: {m}\\n\"\n",
    "    raise SystemExit(msg)\n",
    "\n",
    "\n",
    "\n",
    "raw = pd.concat(frames, ignore_index=True)\n",
    "print(f\"Combined rows from CSV: {len(raw):,}\")\n",
    "print(\"Columns:\", list(raw.columns))\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\nWarnings while reading ({len(errors)} file(s)):\")\n",
    "    for p, msg in errors[:12]:\n",
    "        print(\"  -\", p, \"â†’\", msg)\n",
    "    if len(errors) > 12:\n",
    "        print(\"  ... (truncated)\")\n",
    "\n",
    "print(\"\\nOrg unit mapping started. Please wait...\")\n",
    "\n",
    "# ---- Resolve orgUnit from region_name via CSV map ----\n",
    "org_col = COLS[\"orgunit_source\"]\n",
    "if org_col not in raw.columns:\n",
    "    raise ValueError(f\"Expected column '{org_col}' not found. Present columns: {list(raw.columns)}\")\n",
    "\n",
    "_raw_norm = raw[org_col].astype(str).map(_normalize_key)\n",
    "raw[\"orgUnit_resolved\"] = _raw_norm.map(_orgunit_map)\n",
    "\n",
    "missing_mask = raw[\"orgUnit_resolved\"].isna()\n",
    "if missing_mask.any():\n",
    "    missing_regions = sorted(set(raw.loc[missing_mask, org_col].astype(str).str.strip()))\n",
    "    if STRICT_ORGUNIT_MATCH:\n",
    "        raise KeyError(\n",
    "            f\"{len(missing_regions)} region_name value(s) not found in {ORGUNIT_MAP_CSV}: {missing_regions}\"\n",
    "        )\n",
    "    else:\n",
    "        raw.loc[missing_mask, \"orgUnit_resolved\"] = raw.loc[missing_mask, org_col]\n",
    "\n",
    "print(\"OrgUnit mapping complete.\")\n",
    "print(\"  Resolved:\", (~missing_mask).sum(), \"rows\")\n",
    "print(\"  Unresolved:\", missing_mask.sum(), \"rows\")\n",
    "\n",
    "print(\"\\nPreparing display...\")\n",
    "display(raw.head(5)[ [c for c in (REQUIRED_BASE_COLS + [\"days\"]) if c in raw.columns] + [\"orgUnit_resolved\", \"__source_file\", \"__source_year_folder\"] ])\n",
    "\n",
    "print(\"\\nTask completed successfull.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43ab150",
   "metadata": {},
   "source": [
    "### Step 4: Filter to ICD-10 J-codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68ceca96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering J-codes from 351,019 records. Please wait...\n",
      "Filtered J-codes: 17,655 rows (of 351,019)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12652</th>\n",
       "      <td>J45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269224</th>\n",
       "      <td>J44.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312455</th>\n",
       "      <td>J44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255236</th>\n",
       "      <td>J45.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309759</th>\n",
       "      <td>J03.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          icd\n",
       "12652     J45\n",
       "269224  J44.9\n",
       "312455    J44\n",
       "255236  J45.9\n",
       "309759  J03.9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  --- Step 4: Filter to ICD-10 J-codes\n",
    "\n",
    "print(f\"Filtering J-codes from {len(raw):,} records. Please wait...\")\n",
    "icd_col = COLS[\"icd\"]\n",
    "if icd_col not in raw.columns:\n",
    "    raise ValueError(f\"ICD column '{icd_col}' not in CSV! Columns available: {list(raw.columns)}\")\n",
    "\n",
    "# Normalize ICD to uppercase and strip spaces\n",
    "raw[icd_col] = raw[icd_col].astype(str).str.upper().str.strip()\n",
    "\n",
    "# Keep only J-codes (J00â€“J99.* etc.)\n",
    "j_mask = raw[icd_col].str.match(r\"^J\\d{2}(\\.\\d+)?$\")  # adjust if your data uses different patterns\n",
    "filtered = raw.loc[j_mask].copy()\n",
    "\n",
    "print(f\"Filtered J-codes: {len(filtered):,} rows (of {len(raw):,})\")\n",
    "filtered[[icd_col]].sample(min(5, len(filtered)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a00b9a4",
   "metadata": {},
   "source": [
    "### Step 5 â€” Parse dates & resolve Organization Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b50cded4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing dates & resolving organization units. Please wait...\n",
      "Parsing dates & resolving organization units completed.\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 5 â€” Parse dates to ISO (eventDate) =====\n",
    "\n",
    "print(\"Parsing dates & resolving organization units. Please wait...\")\n",
    "date_col = COLS[\"event_date\"]            # 'admission_date' per Step 1\n",
    "org_col  = COLS[\"orgunit_source\"]        # 'region_name'\n",
    "\n",
    "# Work on a copy\n",
    "filtered = filtered.copy()\n",
    "\n",
    "# Parse admission/discharge dates (keep originals)\n",
    "filtered.loc[:, \"eventDate_iso\"] = pd.to_datetime(\n",
    "    filtered[date_col], errors=\"coerce\", infer_datetime_format=True\n",
    ").dt.date.astype(\"string\")\n",
    "\n",
    "# Optional: also normalize discharge_date if you're storing it as a DE\n",
    "if \"discharge_date\" in filtered.columns:\n",
    "    filtered.loc[:, \"discharge_date_iso\"] = pd.to_datetime(\n",
    "        filtered[\"discharge_date\"], errors=\"coerce\", infer_datetime_format=True\n",
    "    ).dt.date.astype(\"string\")\n",
    "\n",
    "# Final orgUnit is already in raw (Step 3) as orgUnit_resolved; keep it here too\n",
    "if \"orgUnit_resolved\" not in filtered.columns:\n",
    "    raise ValueError(\"Expected 'orgUnit_resolved' from Step 3 is missing.\")\n",
    "\n",
    "print(\"Parsing dates & resolving organization units completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637412ab",
   "metadata": {},
   "source": [
    "### Step 6 â€” Build DHIS2 Event JSON payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "458d44f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing JSON payload. Please wait...\n",
      "Built 17,654 event(s). Skipped 1 row(s) with missing orgUnit/date.\n",
      "[\n",
      "  {\n",
      "    \"event\": \"K1XxGyOtTGM\",\n",
      "    \"program\": \"FG1oe4dXuRQ\",\n",
      "    \"programStage\": \"fhBQ2bdH9Vi\",\n",
      "    \"orgUnit\": \"tmYbVNQz6uy\",\n",
      "    \"eventDate\": \"2020-03-18\",\n",
      "    \"status\": \"COMPLETED\",\n",
      "    \"storedBy\": \"climate-tools-notebook\",\n",
      "    \"dataValues\": [\n",
      "      {\n",
      "        \"dataElement\": \"p7x5goMW59L\",\n",
      "        \"value\": \"Male\"\n",
      "      },\n",
      "      {\n",
      "        \"dataElement\": \"IJwdAM6XTaL\",\n",
      "        \"value\": \"0.4167\"\n",
      "      },\n",
      "      {\n",
      "        \"dataElement\": \"mv9BXIKbqyB\",\n",
      "        \"value\": \"live\"\n",
      "      },\n",
      "      {\n",
      "        \"dataElement\": \"lmwfGS3XTD9\",\n",
      "        \"value\": \"14.0\"\n",
      "      },\n",
      "      {\n",
      "        \"dataElement\": \"VZrKVPTEwyI\",\n",
      "        \"value\": \"J21.9\"\n",
      "      },\n",
      "      {\n",
      "        \"dataElement\": \"FvHnVblNelg\",\n",
      "        \"value\": \"2020-04-01\"\n",
      "      }\n",
      "    ],\n",
      "    \"completedDate\": \"2020-03-18\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 6 â€” Build Event JSON =====\n",
    "\n",
    "print(\"Preparing JSON payload. Please wait...\")\n",
    "\n",
    "def row_to_event(row):\n",
    "    dvs = []\n",
    "    for csv_key, de_uid in DE_MAP.items():\n",
    "        val = row.get(csv_key, row.get(COLS.get(csv_key, csv_key)))\n",
    "        if csv_key == \"sex\":\n",
    "            val = row.get(\"sex_code\", val)  # normalized option code\n",
    "        if csv_key == \"discharge_date\" and \"discharge_date_iso\" in row:\n",
    "            val = row.get(\"discharge_date_iso\", val)\n",
    "        if pd.isna(val):\n",
    "            continue\n",
    "        dvs.append({\"dataElement\": de_uid, \"value\": str(val)})\n",
    "\n",
    "    ev = {\n",
    "        \"event\": row[\"event_client_uid\"],                 # <<â€”â€” stable client UID\n",
    "        \"program\": PROGRAM_ID,\n",
    "        \"programStage\": PROGRAM_STAGE_ID,\n",
    "        \"orgUnit\": row[\"orgUnit_resolved\"],\n",
    "        \"eventDate\": row[\"eventDate_iso\"],\n",
    "        \"status\": DEFAULT_EVENT_STATUS,\n",
    "        \"storedBy\": STORED_BY,\n",
    "        \"dataValues\": dvs,\n",
    "    }\n",
    "    if DEFAULT_EVENT_STATUS == \"COMPLETED\" and pd.notna(row.get(\"eventDate_iso\")):\n",
    "        ev[\"completedDate\"] = str(row[\"eventDate_iso\"])\n",
    "    return ev\n",
    "\n",
    "\n",
    "events = []\n",
    "bad_rows = 0\n",
    "for _, r in filtered.iterrows():\n",
    "    if pd.isna(r.get(\"orgUnit_resolved\")) or pd.isna(r.get(\"eventDate_iso\")):\n",
    "        bad_rows += 1\n",
    "        continue\n",
    "    events.append(row_to_event(r))\n",
    "\n",
    "print(f\"Built {len(events):,} event(s). Skipped {bad_rows:,} row(s) with missing orgUnit/date.\")\n",
    "print(json.dumps(events[:1], indent=2)[:800])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177dd97f-452d-4825-a6dc-da4e6d7c1241",
   "metadata": {},
   "source": [
    "### Step 7: Import events in clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd8044ec-8955-4135-ad79-779540cf6a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting dry-run first. Please wait...\n",
      "Dry-run HTTP: 200 | time: 0:00:02\n",
      "{\n",
      "  \"httpStatus\": \"OK\",\n",
      "  \"httpStatusCode\": 200,\n",
      "  \"status\": \"OK\",\n",
      "  \"message\": \"Import was successful.\",\n",
      "  \"response\": {\n",
      "    \"responseType\": \"ImportSummaries\",\n",
      "    \"status\": \"SUCCESS\",\n",
      "    \"imported\": 0,\n",
      "    \"updated\": 0,\n",
      "    \"deleted\": 0,\n",
      "    \"ignored\": 0,\n",
      "    \"importOptions\": {\n",
      "      \"idSchemes\": {},\n",
      "      \"dryRun\": true,\n",
      "      \"async\": false,\n",
      "      \"importStrategy\": \"CREATE_AND_UPDATE\",\n",
      "      \"mergeMode\": \"REPLACE\",\n",
      "      \"reportMode\": \"FULL\",\n",
      "      \"skipExistingCheck\": false,\n",
      "      \"sharing\": false,\n",
      "      \"skipNotifications\": false,\n",
      "      \"skipAudit\": false,\n",
      "      \"datasetAllowsPeriods\": false,\n",
      "      \"strictPeriods\": false,\n",
      "      \"strictDataElements\": false,\n",
      "      \"strictCategoryOptionCombos\": false,\n",
      "      \"strictAttributeOptionCombos\": false,\n",
      "      \"strictOrganisationUnits\": false,\n",
      "      \"strictDataSetApproval\": false,\n",
      "      \"strictDataSetLocking\": false,\n",
      "      \"strictDataSetInputPeriods\": false,\n",
      "      \"requireCategoryOptionCombo\": false,\n",
      "      \"requireAttributeOptionCombo\": false,\n",
      "      \"skipPatternValidation\": false,\n",
      "      \"ignoreEmptyCollection\": false,\n",
      "      \"force\": false,\n",
      "      \"firstRowIsHeader\": true,\n",
      "      \"skipLastUpdated\": false,\n",
      "      \"mergeDataValues\": false,\n",
      "      \"skipCache\": false\n",
      "    },\n",
      "    \"importSummaries\": [],\n",
      "    \"total\": 0\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceed with real import? (y/N)  N\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped actual import. You can inspect 'events' or re-run without DRY_RUN_FIRST.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 7 â€” Import in clusters (request_timeout=120 + timing/ETA) ---\n",
    "\n",
    "import json, os, time, requests\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# ---------- Inspect a batch response & show conflicts; optionally save failed rows ----------\n",
    "def summarize_import_response(code, out):\n",
    "    \"\"\"Return (totals, conflicts_df) from a DHIS2 /api/events response.\"\"\"\n",
    "    totals = {\"imported\": 0, \"updated\": 0, \"deleted\": 0, \"ignored\": 0}\n",
    "    rows = []\n",
    "\n",
    "    resp = (out or {}).get(\"response\") or {}\n",
    "    summaries = resp.get(\"importSummaries\") or []\n",
    "\n",
    "    for idx, s in enumerate(summaries):\n",
    "        ic = s.get(\"importCount\", {}) or {}\n",
    "        for k in totals:\n",
    "            totals[k] += int(ic.get(k, 0))\n",
    "        for c in (s.get(\"conflicts\") or []):\n",
    "            rows.append({\n",
    "                \"event_idx_in_batch\": idx,\n",
    "                \"object\": c.get(\"object\"),\n",
    "                \"message\": c.get(\"value\")\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return totals, df\n",
    "\n",
    "def show_batch_details(batch_index, start_idx, end_idx, batch, code, out, save_failed=True, base_dir=\"/mnt/data/dhis2_batch_diagnostics\"):\n",
    "    \"\"\"Pretty-print batch result, top conflicts, and save failed rows for re-run.\"\"\"\n",
    "    status = (out or {}).get(\"status\", out.get(\"httpStatusCode\", code) if isinstance(out, dict) else code)\n",
    "    totals, conflicts_df = summarize_import_response(code, out)\n",
    "\n",
    "    print(f\"\\nBatch {batch_index} ({start_idx}-{end_idx}) HTTP {code} ({status})\")\n",
    "    print(\"Import counts:\", totals)\n",
    "\n",
    "    if not conflicts_df.empty:\n",
    "        print(\"\\nTop conflict messages:\")\n",
    "        display(conflicts_df[\"message\"].value_counts().head(15))\n",
    "        print(\"\\nFirst conflicts:\")\n",
    "        display(conflicts_df.head(20))\n",
    "\n",
    "    if save_failed:\n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "        diag_path = os.path.join(base_dir, f\"batch_{batch_index:04d}_response.json\")\n",
    "        with open(diag_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(out, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Identify failed rows\n",
    "        failed_indices = []\n",
    "        resp = (out or {}).get(\"response\") or {}\n",
    "        summaries = resp.get(\"importSummaries\") or []\n",
    "        for i, s in enumerate(summaries):\n",
    "            if (s.get(\"status\") or \"\").upper() in {\"ERROR\", \"WARNING\"}:\n",
    "                failed_indices.append(i)\n",
    "\n",
    "        if failed_indices:\n",
    "            failed_events = [batch[i] for i in failed_indices if i < len(batch)]\n",
    "            failed_path = os.path.join(base_dir, f\"batch_{batch_index:04d}_failed_events.json\")\n",
    "            with open(failed_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump({\"events\": failed_events}, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"\\nSaved diagnostics to:\\n  - {diag_path}\\n  - {failed_path}  (failed {len(failed_events)} / {len(batch)})\")\n",
    "        else:\n",
    "            print(f\"\\nSaved diagnostics to:\\n  - {diag_path}  (no per-event failures listed)\")\n",
    "\n",
    "# ---------- Ensure session exists ----------\n",
    "if \"session\" not in globals():\n",
    "    session = requests.Session()\n",
    "    session.verify = VERIFY_SSL\n",
    "    session.auth = (DHIS2_USERNAME, DHIS2_PASSWORD)\n",
    "    session.headers.update({\"Content-Type\": \"application/json\"})\n",
    "\n",
    "# HTTP timeout (seconds) for each POST\n",
    "request_timeout = 120\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "def import_batch(batch, dry_run=True):\n",
    "    \"\"\"POST /api/events with optional dryRun and request timeout.\"\"\"\n",
    "    url = f\"{DHIS2_BASE_URL}/api/events\"\n",
    "    params = {\"skipFirst\": \"false\", \"dryRun\": str(dry_run).lower()}\n",
    "    payload = {\"events\": batch}\n",
    "    r = session.post(url, params=params, data=json.dumps(payload), timeout=request_timeout)\n",
    "    try:\n",
    "        out = r.json()\n",
    "    except Exception:\n",
    "        out = {\"status_code\": r.status_code, \"text\": r.text[:500]}\n",
    "    return r.status_code, out\n",
    "\n",
    "# Optional: tiny conflict probe\n",
    "def dry_run_debug(n=25):\n",
    "    sample = events[:n]\n",
    "    code, out = import_batch(sample, dry_run=True)\n",
    "    print(\"HTTP:\", code, \"| status:\", out.get(\"status\"), \"| msg:\", out.get(\"message\"))\n",
    "    summaries = (out.get(\"response\") or {}).get(\"importSummaries\", []) or []\n",
    "    rows = []\n",
    "    for i, s in enumerate(summaries):\n",
    "        for c in s.get(\"conflicts\", []) or []:\n",
    "            rows.append({\"event_idx\": i, \"object\": c.get(\"object\"), \"value\": c.get(\"value\")})\n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows)\n",
    "        display(df.head(50))\n",
    "        print(\"\\nTop conflict messages:\")\n",
    "        display(df[\"value\"].value_counts().head(20))\n",
    "    else:\n",
    "        print(\"No conflicts returned.\")\n",
    "    return code, out\n",
    "\n",
    "def fmt_hms(seconds: float) -> str:\n",
    "    return str(timedelta(seconds=int(seconds)))\n",
    "\n",
    "# ---------- 7a) Dry-run a representative sample (up to 1000) ----------\n",
    "print(\"Attempting dry-run first. Please wait...\")\n",
    "if DRY_RUN_FIRST and events:\n",
    "    sample = list(chunks(events, min(BATCH_SIZE, 1000)))[0]\n",
    "    t0 = time.perf_counter()\n",
    "    code, out = import_batch(sample, dry_run=True)\n",
    "    dt = time.perf_counter() - t0\n",
    "    print(\"Dry-run HTTP:\", code, \"| time:\", fmt_hms(dt))\n",
    "    print(json.dumps(out, indent=2)[:1500])\n",
    "\n",
    "# _ = dry_run_debug(30)  # uncomment if you want a tiny conflict probe\n",
    "\n",
    "# ---------- 7b) Actual import with per-batch timing, throughput, ETA, total time ----------\n",
    "import_results = []\n",
    "if events and (not DRY_RUN_FIRST or input(\"Proceed with real import? (y/N) \").strip().lower() == \"y\"):\n",
    "    total = len(events)\n",
    "    overall_start = time.perf_counter()\n",
    "    processed = 0\n",
    "\n",
    "    for i, batch in enumerate(chunks(events, BATCH_SIZE), start=1):\n",
    "        start_idx = (i - 1) * BATCH_SIZE + 1\n",
    "        end_idx   = min(i * BATCH_SIZE, total)\n",
    "\n",
    "        batch_start = time.perf_counter()\n",
    "        code, out = import_batch(batch, dry_run=False)\n",
    "        batch_dt = time.perf_counter() - batch_start\n",
    "\n",
    "        status = out.get(\"status\", out.get(\"httpStatusCode\", code))\n",
    "        processed += len(batch)\n",
    "\n",
    "        # Throughput + ETA\n",
    "        overall_dt = time.perf_counter() - overall_start\n",
    "        eps = processed / overall_dt if overall_dt > 0 else 0.0\n",
    "        remaining = total - processed\n",
    "        eta_sec = remaining / eps if eps > 0 else 0.0\n",
    "\n",
    "        print(\n",
    "            f\"Batch {i}: {start_idx}-{end_idx} of {total} events â†’ \"\n",
    "            f\"HTTP {code} ({status}) | batch {fmt_hms(batch_dt)} | \"\n",
    "            f\"avg {eps:.1f} ev/s | ETA {fmt_hms(eta_sec)}\"\n",
    "        )\n",
    "\n",
    "        # inside your Step 7 loop after a successful batch:\n",
    "        with open(\"/mnt/data/import_checkpoint.txt\", \"w\") as f:\n",
    "            f.write(str(end_idx))  # last 1-based event index processed\n",
    "\n",
    "\n",
    "        # On error/conflict, show details and save failed rows\n",
    "        if code >= 400 or str(status).upper() in {\"ERROR\", \"CONFLICT\"}:\n",
    "            show_batch_details(i, start_idx, end_idx, batch, code, out, save_failed=True)\n",
    "\n",
    "        import_results.append((i, start_idx, end_idx, len(batch), code, status, batch_dt))\n",
    "\n",
    "    overall_dt = time.perf_counter() - overall_start\n",
    "    print(f\"\\nAll done. Total time: {fmt_hms(overall_dt)} \"\n",
    "          f\"({total} events, avg {(total/overall_dt):.1f} ev/s).\")\n",
    "else:\n",
    "    print(\"Skipped actual import. You can inspect 'events' or re-run without DRY_RUN_FIRST.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9af9993",
   "metadata": {},
   "source": [
    "### Step 8: Post-import sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ba69a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"pager\": {\n",
      "    \"page\": 1,\n",
      "    \"pageSize\": 5,\n",
      "    \"isLastPage\": false\n",
      "  },\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"programStage\": \"fhBQ2bdH9Vi\",\n",
      "      \"programType\": \"WITHOUT_REGISTRATION\",\n",
      "      \"orgUnit\": \"rrPMoH1VER7\",\n",
      "      \"program\": \"FG1oe4dXuRQ\",\n",
      "      \"event\": \"jWERIzfIcEQ\",\n",
      "      \"status\": \"COMPLETED\",\n",
      "      \"orgUnitName\": \"Gampaha RDHS\",\n",
      "      \"eventDate\": \"2020-04-01T00:00:00.000\",\n",
      "      \"created\": \"2025-10-17T03:24:14.143\",\n",
      "      \"lastUpdated\": \"2025-10-17T03:24:14.143\",\n",
      "      \"deleted\": false,\n",
      "      \"attributeOptionCombo\": \"HllvX50cXC0\",\n",
      "      \"dataValues\": [\n",
      "        {\n",
      "          \"lastUpdated\": \"2025-10-17T03:24:09.297\",\n",
      "          \"created\": \"2025-10-17T03:24:09.297\",\n",
      "          \"dataElement\": \"IJwdAM6XTaL\",\n",
      "          \"value\": \"1.1667\",\n",
      "          \"providedElsewhere\": false,\n",
      "          \"createdByUserInfo\": {\n",
      "            \"uid\": \"qtojH98PIy9\",\n",
      "            \"firstName\": \"HISP\",\n",
      "            \"surname\": \"Sri Lanka\",\n",
      "            \"username\": \"hispsl\"\n",
      "          },\n",
      "          \"lastUpdatedByUserInfo\": {\n",
      "            \"uid\": \"qtojH98PIy9\",\n",
      "            \"firstName\": \"HISP\",\n",
      "            \"surname\": \"Sri Lanka\",\n",
      "            \"username\": \"hispsl\"\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"lastUpdated\": \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# You can GET a small sample back to verify\n",
    "def get_recent_events(limit=5):\n",
    "    url = f\"{DHIS2_BASE_URL}/api/events.json\"\n",
    "    params = {\n",
    "        \"program\": PROGRAM_ID,\n",
    "        \"pageSize\": limit,\n",
    "        \"order\": \"created:desc\",\n",
    "        \"paging\": \"true\"\n",
    "    }\n",
    "    r = session.get(url, params=params)\n",
    "    return r.json()\n",
    "\n",
    "try:\n",
    "    sample_events = get_recent_events(5)\n",
    "    print(json.dumps(sample_events, indent=2)[:1200])\n",
    "except Exception as e:\n",
    "    print(\"Skipping fetch:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73fd549",
   "metadata": {},
   "source": [
    "## Notes & Tips\n",
    "- Ensure your **program** is an *Event* program (not Tracker) and the **program stage** allows eventDate.\n",
    "- DataElements in `DE_MAP` must belong to the program stage.\n",
    "- If your org units are by CODE, set `ORGUNIT_ID_SCHEME = 'CODE'` and supply codes in `orgUnit_resolved`.\n",
    "- If you see `conflicts` in the import summary, check date formats, missing org units, or wrong DE UIDs.\n",
    "- For performance, keep `BATCH_SIZE` between 200â€“1000 depending on your server.\n",
    "- If your server has a self-signed certificate, set `VERIFY_SSL=False` (only if you understand the risk).\n",
    "- To transform columns before mapping (e.g., normalize sex M/F), create new columns and reference them in `DE_MAP`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
