{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217827ea",
   "metadata": {},
   "source": [
    "---\n",
    "title: Downloading and Harmonizing Dengue Data from OpenDengue\n",
    "short_title: Dengue Cases\n",
    "---\n",
    "\n",
    "This guide demonstrates how to download and harmonize [**OpenDengue**](https://opendengue.org/data.html) case data for use with DHIS2. The same approach can also be applied to local Dengue case counts from official Ministry of Health data.\n",
    "\n",
    "The notebook focuses on **data harmonization and preparation** using a worked example for **Nepal (districts / admin2)** and **monthly** data.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "This workflow expects two local input files under `../../guides/data/`:\n",
    "\n",
    "- `nepal-opendengue.csv` — [**OpenDengue**](https://opendengue.org/data.html) export containing Nepal dengue case counts\n",
    "- `nepal-locations.geojson` — Nepal district geometries (admin2)\n",
    "\n",
    "## Output\n",
    "\n",
    "The workflow produces:\n",
    "\n",
    "- `nepal-dengue-harmonized.csv` — harmonized monthly dengue cases per district (`time_period`, `location`, `disease_cases`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ff57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d837eb6",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b539e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path(\"../../guides/data\")\n",
    "\n",
    "LOCATIONS_GEOJSON = DATA_FOLDER / \"nepal-locations.geojson\"\n",
    "OPENDENGUE_SOURCE_PATH = DATA_FOLDER / \"nepal-opendengue.csv\"\n",
    "\n",
    "# Output\n",
    "OUT_CSV = DATA_FOLDER / \"nepal-dengue-harmonized.csv\"\n",
    "\n",
    "for p in [LOCATIONS_GEOJSON, OPENDENGUE_SOURCE_PATH]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing required input: {p}\")\n",
    "\n",
    "print(\"Using inputs:\")\n",
    "print(\" -\", LOCATIONS_GEOJSON)\n",
    "print(\" -\", OPENDENGUE_SOURCE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa838c57",
   "metadata": {},
   "source": [
    "## Load district locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = gpd.read_file(LOCATIONS_GEOJSON)\n",
    "\n",
    "# DHIS2 UID \n",
    "uid_col = \"id\" if \"id\" in locations.columns else None\n",
    "if uid_col is None:\n",
    "    raise KeyError(f\"Expected DHIS2 UID in GeoJSON 'id'. Found: {list(locations.columns)}\")\n",
    "\n",
    "locations[\"location\"] = locations[uid_col].astype(str).str.strip() \n",
    "\n",
    "# Join helper (district name)\n",
    "if \"name\" not in locations.columns:\n",
    "    raise KeyError(f\"Expected district name in GeoJSON 'name'. Found: {list(locations.columns)}\")\n",
    "\n",
    "locations[\"district_name\"] = (\n",
    "    locations[\"name\"].astype(str)\n",
    "    .str.replace(r\"^\\s*\\d+\\s+\", \"\", regex=True)  # drop \"101 that came with location names\"\n",
    "    .str.upper()\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Keep only what we need\n",
    "locations = locations[[\"location\", \"district_name\", \"geometry\"]].dropna(subset=[\"location\"]).copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb287b",
   "metadata": {},
   "source": [
    "## Load OpenDengue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8490bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(OPENDENGUE_SOURCE_PATH)\n",
    "print(\"Loaded:\", OPENDENGUE_SOURCE_PATH)\n",
    "print(\"Columns:\", df_raw.columns.tolist())\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d7236b",
   "metadata": {},
   "source": [
    "OpenDengue contains multiple administrative levels in the same file, so we subset to only the Admin 2 units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm2 = df_raw[df_raw['S_res']=='Admin2']\n",
    "print('Number of rows after filtering to admin2 units:', len(df_adm2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db9e6bd",
   "metadata": {},
   "source": [
    "## Column mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b1673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenDengue export columns\n",
    "DATE_COL = \"calendar_start_date\"\n",
    "CASES_COL = \"dengue_total\"\n",
    "ADMIN2_COL = \"adm_2_name\"\n",
    "\n",
    "missing = [c for c in [DATE_COL, CASES_COL, ADMIN2_COL] if c not in df_raw.columns]\n",
    "if missing:\n",
    "    raise KeyError(\n",
    "        f\"Input CSV is missing required columns: {missing}. \"\n",
    "        f\"Available columns: {df_raw.columns.tolist()}\"\n",
    "    )\n",
    "\n",
    "print(\"Using columns:\", {\"date\": DATE_COL, \"cases\": CASES_COL, \"admin2\": ADMIN2_COL})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5bf4c5",
   "metadata": {},
   "source": [
    "## Normalize OpenDengue (Nepal districts / admin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56407342",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = pd.DataFrame({\n",
    "    \"date\": pd.to_datetime(df_raw[DATE_COL], errors=\"coerce\"),\n",
    "    \"cases\": pd.to_numeric(df_raw[CASES_COL], errors=\"coerce\"),\n",
    "    \"district_name\": df_raw[ADMIN2_COL],   # <-- not location yet\n",
    "})\n",
    "\n",
    "# Normalize district name for the crosswalk join\n",
    "df_norm[\"district_name\"] = (\n",
    "    df_norm[\"district_name\"]\n",
    "    .astype(str)\n",
    "    .str.upper()\n",
    "    .str.strip()\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    ")\n",
    "\n",
    "# Keep only valid rows\n",
    "# Map district_name -> DHIS2 orgUnit UID\n",
    "df_norm = df_norm.merge(\n",
    "    locations[[\"district_name\", \"location\"]],\n",
    "    on=\"district_name\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Fail fast (or drop) if mapping is incomplete\n",
    "unmapped = df_norm[\"location\"].isna().mean()\n",
    "print(f\"Unmapped dengue rows: {unmapped:.2%}\")\n",
    "if unmapped > 0:\n",
    "    print(\"Examples:\", df_norm.loc[df_norm[\"location\"].isna(), \"district_name\"].drop_duplicates().head(15).tolist())\n",
    "\n",
    "df_norm = df_norm.dropna(subset=[\"location\"]).copy()\n",
    "\n",
    "\n",
    "df_norm = df_norm.dropna(subset=[\"date\", \"cases\", \"district_name\"])\n",
    "df_norm = df_norm[df_norm[\"district_name\"].ne(\"\")]\n",
    "\n",
    "df_norm.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8657935",
   "metadata": {},
   "source": [
    "## Monthly aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20572e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to month period label (YYYY-MM)\n",
    "df_norm[\"time_period\"] = df_norm[\"date\"].dt.to_period(\"M\").astype(str)\n",
    "\n",
    "# Aggregate within month + location\n",
    "disease = (\n",
    "    df_norm.groupby([\"time_period\", \"location\"], as_index=False)[\"cases\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"cases\": \"disease_cases\"})\n",
    ")\n",
    "\n",
    "print(\"Aggregated rows:\", len(disease))\n",
    "disease.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec13e34",
   "metadata": {},
   "source": [
    "## Filter to spatial backbone and align time axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975defcc",
   "metadata": {},
   "source": [
    "## Filter to districts and align time axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c14efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only locations present in the GeoJSON backbone\n",
    "before = len(disease)\n",
    "disease = disease.merge(locations[[\"location\"]], on=\"location\", how=\"inner\")\n",
    "after = len(disease)\n",
    "print(f\"Backbone filter kept {after}/{before} rows\")\n",
    "\n",
    "# Build full (time_period x location) grid and fill missing with 0\n",
    "all_months = pd.period_range(disease[\"time_period\"].min(), disease[\"time_period\"].max(), freq=\"M\").astype(str)\n",
    "all_locations = locations[\"location\"].sort_values().unique()\n",
    "\n",
    "grid = pd.MultiIndex.from_product([all_months, all_locations], names=[\"time_period\", \"location\"]).to_frame(index=False)\n",
    "\n",
    "disease_full = grid.merge(disease, on=[\"time_period\", \"location\"], how=\"left\")\n",
    "disease_full[\"disease_cases\"] = disease_full[\"disease_cases\"].fillna(0)\n",
    "\n",
    "# Keep integer-looking values as ints where possible\n",
    "disease_full[\"disease_cases\"] = pd.to_numeric(disease_full[\"disease_cases\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "print(\"Final rows (complete grid):\", len(disease_full))\n",
    "disease_full.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526a22ae",
   "metadata": {},
   "source": [
    "## Write output CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02edd99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_full.to_csv(OUT_CSV, index=False)\n",
    "print(\"Wrote:\", OUT_CSV)\n",
    "OUT_CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c4c779",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f3b8f8",
   "metadata": {},
   "source": [
    "This guide stops after downloading and producing a harmonized, DHIS2-ready dataset.\n",
    "\n",
    "To import the resulting data into DHIS2, see our guides for [importing data to DHIS2](../../import-data/intro.md).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate-tools-py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
