{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e25381",
   "metadata": {},
   "source": [
    "---\n",
    "title: Harmonize climate and health data into a Chap-ready CSV file\n",
    "short_title: Prepare data for Chap\n",
    "---\n",
    "\n",
    "This notebook demonstrates **orchestration and harmonization across multiple sources** to produce a single,\n",
    "modelling-ready CSV files compatible with the [DHIS2 Chap Modeling Platform](https://chap.dhis2.org/). \n",
    "\n",
    "**Scope (important):**\n",
    "- We **consume harmonized outputs** produced by the earlier workflow notebooks (OpenDengue, WorldPop, ERA5, CHIRPS3).\n",
    "- We do **not** repeat source-specific harmonization choices here (e.g. filtering mixed resolutions in OpenDengue).\n",
    "- Our job in this notebook is to align all inputs onto a shared **modelling grid**: *(admin unit × monthly time_period)*,\n",
    "  merge them, and export a single CSV.\n",
    "\n",
    "Missing values are preserved as `NaN` (no imputation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb2915b",
   "metadata": {},
   "source": [
    "## Spatial harmonization\n",
    "\n",
    "To merge heterogeneous sources into a single modelling table, we must choose a\n",
    "common spatial unit (the modelling geography) and express all inputs on that unit.\n",
    "\n",
    "In this example, we use administrative units from `data/nepal-locations.geojson` as\n",
    "the **spatial spine**. All datasets must map to these units before they can be merged.\n",
    "\n",
    "| Dataset      | Native spatial resolution            | Harmonized resolution | Notes |\n",
    "|-------------|--------------------------------------|-----------------------|-------|\n",
    "| OpenDengue  | Mixed (Admin0/Admin1/Admin2)          | Admin units (GeoJSON) | We **consume the dengue-harmonized output** from the dengue workflow, already mapped to the chosen admin level. |\n",
    "| ERA5-Land   | Regular grid (~0.1°)                  | Admin units (GeoJSON) | We **consume the admin-level output** from the ERA5 workflow (already reduced over polygons). |\n",
    "| CHIRPS3     | Regular grid (~0.05°)                 | Admin units (GeoJSON) | We **consume the admin-level output** from the CHIRPS3 workflow (already reduced over polygons). |\n",
    "| WorldPop    | Raster grid (≈100m–1km, product dependent) | Admin units (GeoJSON) | We **consume the admin-level output** from the WorldPop workflow (aggregated by polygon sum). |\n",
    "\n",
    "Spatial harmonization aligns all sources to a shared modelling geography; it does **not**\n",
    "increase the native spatial precision of any source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05795b4",
   "metadata": {},
   "source": [
    "## Temporal harmonization\n",
    "\n",
    "To merge heterogeneous data sources into a single modelling table, we must choose a\n",
    "common temporal resolution (the modelling clock) and express all inputs on that axis.\n",
    "\n",
    "In this example, we use a **monthly** time step.\n",
    "\n",
    "| Dataset      | Native temporal resolution        | Harmonized resolution | Notes |\n",
    "|-------------|----------------------------------|-----------------------|-------|\n",
    "| OpenDengue  | Weekly / irregular                | Monthly               | We **consume the dengue-harmonized output** from the dengue workflow, aggregated to one value per month and location. |\n",
    "| ERA5-Land   | Hourly / daily                    | Monthly               | We consume daily (or monthly) admin-level outputs and aggregate to monthly where needed. |\n",
    "| CHIRPS3     | Daily                             | Monthly               | We consume daily admin-level outputs and aggregate to monthly where needed. |\n",
    "| WorldPop    | Yearly (static)                   | Monthly (expanded)    | We consume the upstream output where yearly totals are aggregated to admin units and expanded to monthly. |\n",
    "\n",
    "Temporal harmonization aligns all datasets on the same time axis; it does **not**\n",
    "increase the intrinsic temporal precision of any source. Static or sparsely sampled\n",
    "datasets remain static after alignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e972050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Parameters\n",
    "# ---------------------------------------------------------------------\n",
    "FREQ = \"monthly\"\n",
    "\n",
    "# This notebook consumes harmonized outputs produced by earlier workflow notebooks.\n",
    "DATA_DIR = Path(\"../data\").resolve()\n",
    "\n",
    "# Harmonized inputs (already aligned to the same orgunit and monthly time_period)\n",
    "DENGUE_CSV = DATA_DIR / \"nepal-dengue-monthly-admin.csv\"      # columns: location, time_period, disease_cases\n",
    "POP_CSV    = DATA_DIR / \"nepal-worldpop-monthly-admin.csv\"    # columns: location, time_period, population\n",
    "PRCP_CSV   = DATA_DIR / \"nepal-era5-prcp-monthly-admin.csv\"   # columns: location, time_period, precip_mm\n",
    "T2M_CSV    = DATA_DIR / \"nepal-era5-t2m-monthly-admin.csv\"    # columns: location, time_period, t2m_c\n",
    "\n",
    "# Output\n",
    "OUTPUT_CSV = DATA_DIR / \"nepal_dengue_pop_climate_chap.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f7b39",
   "metadata": {},
   "source": [
    "## Helper utilities\n",
    "Before merging sources, we apply a small set of checks to ensure all inputs already conform to the expected modelling contract (location, time_period, monthly resolution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd8dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Helpers\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "PERIOD_YYYYMM = re.compile(r\"^\\d{6}$\")\n",
    "\n",
    "def read_csv(path: Path, required: list[str]) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing input: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"{path.name} missing {missing}. Found: {list(df.columns)}\")\n",
    "\n",
    "    # normalize keys\n",
    "    df[\"location\"] = df[\"location\"].astype(str).str.strip()\n",
    "    df[\"time_period\"] = (\n",
    "        df[\"time_period\"].astype(str).str.replace(r\"\\D\", \"\", regex=True).str.zfill(6)\n",
    "    )\n",
    "\n",
    "    bad = ~df[\"time_period\"].str.match(PERIOD_YYYYMM)\n",
    "    if bad.any():\n",
    "        raise ValueError(f\"Invalid time_period (YYYYMM). Examples: {df.loc[bad, 'time_period'].head(5).tolist()}\")\n",
    "\n",
    "    df = df.dropna(subset=[\"location\", \"time_period\"]).copy()\n",
    "\n",
    "    if df.duplicated([\"location\", \"time_period\"]).any():\n",
    "        raise ValueError(f\"{path.name} has duplicate (location, time_period) rows\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def month_index(start_yyyymm: str, end_yyyymm: str) -> pd.Index:\n",
    "    return pd.period_range(start_yyyymm, end_yyyymm, freq=\"M\").astype(str).str.replace(\"-\", \"\", regex=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f3f95f",
   "metadata": {},
   "source": [
    "## Load inputs (harmonized outputs)\n",
    "\n",
    "At this point, the earlier workflow notebooks should have produced:\n",
    "- a dengue table at **monthly** resolution (`location`, `time_period`, `disease_cases`)\n",
    "- an admin-level WorldPop table (`location`, population)\n",
    "- admin-level climate time series for ERA5 (and optionally CHIRPS3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a7251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load harmonized inputs (already monthly, already on the admin-unit spine)\n",
    "dengue = read_csv(DENGUE_CSV, [\"location\", \"time_period\", \"disease_cases\"])\n",
    "pop    = read_csv(POP_CSV,    [\"location\", \"time_period\", \"population\"])\n",
    "prcp   = read_csv(PRCP_CSV,   [\"location\", \"time_period\", \"tp\"])\n",
    "t2m    = read_csv(T2M_CSV,    [\"location\", \"time_period\", \"t2m_c\"])\n",
    "\n",
    "# Coerce numeric columns (preserve missing values as NaN)\n",
    "dengue[\"disease_cases\"] = pd.to_numeric(dengue[\"disease_cases\"], errors=\"coerce\")\n",
    "pop[\"population\"]       = pd.to_numeric(pop[\"population\"], errors=\"coerce\")\n",
    "prcp[\"tp\"]       = pd.to_numeric(prcp[\"tp\"], errors=\"coerce\")\n",
    "t2m[\"t2m_c\"]            = pd.to_numeric(t2m[\"t2m_c\"], errors=\"coerce\")\n",
    "\n",
    "# Quick shape summary\n",
    "summary = pd.DataFrame({\n",
    "    \"rows\": [len(dengue), len(pop), len(prcp), len(t2m)],\n",
    "    \"locations\": [dengue[\"location\"].nunique(), pop[\"location\"].nunique(), prcp[\"location\"].nunique(), t2m[\"location\"].nunique()],\n",
    "    \"months\": [dengue[\"time_period\"].nunique(), pop[\"time_period\"].nunique(), prcp[\"time_period\"].nunique(), t2m[\"time_period\"].nunique()],\n",
    "}, index=[\"dengue\", \"population\", \"precip\", \"t2m\"])\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6171acc6",
   "metadata": {},
   "source": [
    "## Build the modelling grid (admin unit × monthly time_period)\n",
    "\n",
    "We use the dengue time range as the default modelling window, since dengue is the target outcome.\n",
    "\n",
    "All locations from the spatial spine are included. Missing values are preserved as `NaN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dbac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine global modelling window from dengue (target outcome)\n",
    "start_tp = dengue[\"time_period\"].min()\n",
    "end_tp = dengue[\"time_period\"].max()\n",
    "\n",
    "months = month_index(start_tp, end_tp)\n",
    "\n",
    "# Spatial spine: union of all locations present across harmonized inputs\n",
    "locations = pd.Index(\n",
    "    pd.unique(pd.concat([\n",
    "        dengue[\"location\"],\n",
    "        pop[\"location\"],\n",
    "        prcp[\"location\"],\n",
    "        t2m[\"location\"],\n",
    "    ], ignore_index=True)).astype(str),\n",
    "    name=\"location\"\n",
    ").sort_values()\n",
    "\n",
    "# Full modelling grid: all locations × all months\n",
    "grid = pd.MultiIndex.from_product([locations, months], names=[\"location\", \"time_period\"]).to_frame(index=False)\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de656620",
   "metadata": {},
   "source": [
    "## Merge all sources onto the modelling grid\n",
    "\n",
    "We join everything onto the modelling grid using `(location, time_period)`.\n",
    "\n",
    "- Dengue: monthly outcome (`disease_cases`)\n",
    "- Population: monthly covariate (`population`)\n",
    "- Climate: monthly covariates (`tp`, `t2m_c`)\n",
    "\n",
    "Missing values are preserved as `NaN` (no imputation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e8e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the grid\n",
    "df = grid.copy()\n",
    "\n",
    "# Outcome + covariates (all keyed by location + time_period)\n",
    "df = df.merge(dengue, on=[\"location\", \"time_period\"], how=\"left\")\n",
    "df = df.merge(pop,    on=[\"location\", \"time_period\"], how=\"left\")\n",
    "df = df.merge(prcp,   on=[\"location\", \"time_period\"], how=\"left\")\n",
    "df = df.merge(t2m,    on=[\"location\", \"time_period\"], how=\"left\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9851103",
   "metadata": {},
   "source": [
    "## Structural sanity checks\n",
    "\n",
    "We avoid deep data validation in this workflow. These checks confirm that the merged table\n",
    "satisfies the structural assumptions expected by downstream modelling (in our case CHAP).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae237a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One row per (location, time_period)\n",
    "assert not df.duplicated([\"location\", \"time_period\"]).any(), \"Duplicate (location, time_period) rows found.\"\n",
    "\n",
    "# Required Chap fields present\n",
    "required_cols = {\"location\", \"time_period\", \"disease_cases\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "assert not missing, f\"Missing required columns: {missing}\"\n",
    "\n",
    "# Missingness summary (expected; no thresholds enforced here)\n",
    "df.isna().mean().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5806877b",
   "metadata": {},
   "source": [
    "## Export to a Chap-compatible CSV\n",
    "\n",
    "We use the Chap CSV exporter from `dhis2eo`. Reserved fields:\n",
    "- `time_period`, `location`, `disease_cases`\n",
    "- optional reserved field: `population`\n",
    "\n",
    "All other columns are treated as covariates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68571d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dhis2eo.integrations.chap import dataframe_to_chap_csv\n",
    "\n",
    "column_map = {\n",
    "    \"time_period\": \"time_period\",\n",
    "    \"location\": \"location\",\n",
    "    \"disease_cases\": \"disease_cases\",\n",
    "    \"population\": \"population\",   # optional but recommended\n",
    "}\n",
    "\n",
    "dataframe_to_chap_csv(\n",
    "    df=df,\n",
    "    output_path=OUTPUT_CSV,\n",
    "    freq=\"monthly\",\n",
    "    column_map=column_map,\n",
    ")\n",
    "\n",
    "OUTPUT_CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90f9bf8",
   "metadata": {},
   "source": [
    "To inspect the contents of the final CSV file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c96ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chap = pd.read_csv(OUTPUT_CSV)\n",
    "df_chap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dfff27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate-tools-py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
