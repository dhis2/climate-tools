{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217827ea",
   "metadata": {},
   "source": [
    "---\n",
    "title: Full workflow for importing Dengue data into DHIS2\n",
    "short_title: Import Dengue Data\n",
    "---\n",
    "\n",
    "This workflow demonstrates the end-to-end preparation of importing dengue case data into DHIS2. We demonstrate the workflow using [**OpenDengue**](https://opendengue.org/data.html) data, otherwise it is expected for countries to use official Ministry of Health data.\n",
    "\n",
    "The notebook focuses on **data harmonization and preparation** using a worked example for **Nepal (districts / admin2)** and **monthly** data. The final DHIS2 import step follows the same approach as the WorldPop and CHIRPS workflows and is therefore not repeated in full here.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "This workflow expects two local input files under `../../guides/data/`:\n",
    "\n",
    "- `nepal-opendengue.csv` — [**OpenDengue**](https://opendengue.org/data.html) export containing Nepal dengue case counts\n",
    "- `nepal-locations.geojson` — Nepal district geometries (admin2)\n",
    "\n",
    "## Output\n",
    "\n",
    "The workflow produces:\n",
    "\n",
    "- `nepal-dengue-harmonized.csv` — harmonized monthly dengue cases per district (`time_period`, `location`, `disease_cases`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e35ff57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d837eb6",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b539e19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using inputs:\n",
      " - ../../guides/data/nepal-locations.geojson\n",
      " - ../../guides/data/nepal-opendengue.csv\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = Path(\"../../guides/data\")\n",
    "\n",
    "LOCATIONS_GEOJSON = DATA_FOLDER / \"nepal-locations.geojson\"\n",
    "OPENDENGUE_SOURCE_PATH = DATA_FOLDER / \"nepal-opendengue.csv\"\n",
    "\n",
    "# Output\n",
    "OUT_CSV = DATA_FOLDER / \"nepal-dengue-harmonized.csv\"\n",
    "\n",
    "for p in [LOCATIONS_GEOJSON, OPENDENGUE_SOURCE_PATH]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing required input: {p}\")\n",
    "\n",
    "print(\"Using inputs:\")\n",
    "print(\" -\", LOCATIONS_GEOJSON)\n",
    "print(\" -\", OPENDENGUE_SOURCE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa838c57",
   "metadata": {},
   "source": [
    "## Load district locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = gpd.read_file(LOCATIONS_GEOJSON)\n",
    "\n",
    "# DHIS2 UID \n",
    "uid_col = \"id\" if \"id\" in locations.columns else None\n",
    "if uid_col is None:\n",
    "    raise KeyError(f\"Expected DHIS2 UID in GeoJSON 'id'. Found: {list(locations.columns)}\")\n",
    "\n",
    "locations[\"location\"] = locations[uid_col].astype(str).str.strip() \n",
    "\n",
    "# Join helper (district name)\n",
    "if \"name\" not in locations.columns:\n",
    "    raise KeyError(f\"Expected district name in GeoJSON 'name'. Found: {list(locations.columns)}\")\n",
    "\n",
    "locations[\"district_name\"] = (\n",
    "    locations[\"name\"].astype(str)\n",
    "    .str.replace(r\"^\\s*\\d+\\s+\", \"\", regex=True)  # drop \"101 that came with location names\"\n",
    "    .str.upper()\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Keep only what we need\n",
    "locations = locations[[\"location\", \"district_name\", \"geometry\"]].dropna(subset=[\"location\"]).copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb287b",
   "metadata": {},
   "source": [
    "## Load OpenDengue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8490bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: ../../guides/data/nepal-opendengue.csv\n",
      "Columns: ['adm_0_name', 'adm_1_name', 'adm_2_name', 'full_name', 'ISO_A0', 'FAO_GAUL_code', 'RNE_iso_code', 'IBGE_code', 'calendar_start_date', 'calendar_end_date', 'Year', 'dengue_total', 'case_definition_standardised', 'S_res', 'T_res', 'UUID', 'region']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adm_0_name</th>\n",
       "      <th>adm_1_name</th>\n",
       "      <th>adm_2_name</th>\n",
       "      <th>full_name</th>\n",
       "      <th>ISO_A0</th>\n",
       "      <th>FAO_GAUL_code</th>\n",
       "      <th>RNE_iso_code</th>\n",
       "      <th>IBGE_code</th>\n",
       "      <th>calendar_start_date</th>\n",
       "      <th>calendar_end_date</th>\n",
       "      <th>Year</th>\n",
       "      <th>dengue_total</th>\n",
       "      <th>case_definition_standardised</th>\n",
       "      <th>S_res</th>\n",
       "      <th>T_res</th>\n",
       "      <th>UUID</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEPAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEPAL</td>\n",
       "      <td>NPL</td>\n",
       "      <td>175</td>\n",
       "      <td>NPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1987-01-01</td>\n",
       "      <td>1987-12-31</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>Total</td>\n",
       "      <td>Admin0</td>\n",
       "      <td>Year</td>\n",
       "      <td>WHOSEARO-ALL-19852009-Y01-00</td>\n",
       "      <td>SEARO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NEPAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEPAL</td>\n",
       "      <td>NPL</td>\n",
       "      <td>175</td>\n",
       "      <td>NPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1985-01-01</td>\n",
       "      <td>1985-12-31</td>\n",
       "      <td>1985</td>\n",
       "      <td>0</td>\n",
       "      <td>Total</td>\n",
       "      <td>Admin0</td>\n",
       "      <td>Year</td>\n",
       "      <td>WHOSEARO-ALL-19852009-Y01-00</td>\n",
       "      <td>SEARO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEPAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEPAL</td>\n",
       "      <td>NPL</td>\n",
       "      <td>175</td>\n",
       "      <td>NPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1986-01-01</td>\n",
       "      <td>1986-12-31</td>\n",
       "      <td>1986</td>\n",
       "      <td>0</td>\n",
       "      <td>Total</td>\n",
       "      <td>Admin0</td>\n",
       "      <td>Year</td>\n",
       "      <td>WHOSEARO-ALL-19852009-Y01-00</td>\n",
       "      <td>SEARO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEPAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEPAL</td>\n",
       "      <td>NPL</td>\n",
       "      <td>175</td>\n",
       "      <td>NPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991-01-01</td>\n",
       "      <td>1991-12-31</td>\n",
       "      <td>1991</td>\n",
       "      <td>0</td>\n",
       "      <td>Total</td>\n",
       "      <td>Admin0</td>\n",
       "      <td>Year</td>\n",
       "      <td>WHOSEARO-ALL-19852009-Y01-00</td>\n",
       "      <td>SEARO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEPAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEPAL</td>\n",
       "      <td>NPL</td>\n",
       "      <td>175</td>\n",
       "      <td>NPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1988-01-01</td>\n",
       "      <td>1988-12-31</td>\n",
       "      <td>1988</td>\n",
       "      <td>0</td>\n",
       "      <td>Total</td>\n",
       "      <td>Admin0</td>\n",
       "      <td>Year</td>\n",
       "      <td>WHOSEARO-ALL-19852009-Y01-00</td>\n",
       "      <td>SEARO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  adm_0_name adm_1_name adm_2_name full_name ISO_A0  FAO_GAUL_code  \\\n",
       "0      NEPAL        NaN        NaN     NEPAL    NPL            175   \n",
       "1      NEPAL        NaN        NaN     NEPAL    NPL            175   \n",
       "2      NEPAL        NaN        NaN     NEPAL    NPL            175   \n",
       "3      NEPAL        NaN        NaN     NEPAL    NPL            175   \n",
       "4      NEPAL        NaN        NaN     NEPAL    NPL            175   \n",
       "\n",
       "  RNE_iso_code  IBGE_code calendar_start_date calendar_end_date  Year  \\\n",
       "0          NPL        NaN          1987-01-01        1987-12-31  1987   \n",
       "1          NPL        NaN          1985-01-01        1985-12-31  1985   \n",
       "2          NPL        NaN          1986-01-01        1986-12-31  1986   \n",
       "3          NPL        NaN          1991-01-01        1991-12-31  1991   \n",
       "4          NPL        NaN          1988-01-01        1988-12-31  1988   \n",
       "\n",
       "   dengue_total case_definition_standardised   S_res T_res  \\\n",
       "0             0                        Total  Admin0  Year   \n",
       "1             0                        Total  Admin0  Year   \n",
       "2             0                        Total  Admin0  Year   \n",
       "3             0                        Total  Admin0  Year   \n",
       "4             0                        Total  Admin0  Year   \n",
       "\n",
       "                           UUID region  \n",
       "0  WHOSEARO-ALL-19852009-Y01-00  SEARO  \n",
       "1  WHOSEARO-ALL-19852009-Y01-00  SEARO  \n",
       "2  WHOSEARO-ALL-19852009-Y01-00  SEARO  \n",
       "3  WHOSEARO-ALL-19852009-Y01-00  SEARO  \n",
       "4  WHOSEARO-ALL-19852009-Y01-00  SEARO  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv(OPENDENGUE_SOURCE_PATH)\n",
    "print(\"Loaded:\", OPENDENGUE_SOURCE_PATH)\n",
    "print(\"Columns:\", df_raw.columns.tolist())\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db9e6bd",
   "metadata": {},
   "source": [
    "## Column mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "650b1673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using columns: {'date': 'calendar_start_date', 'cases': 'dengue_total', 'admin2': 'adm_2_name'}\n"
     ]
    }
   ],
   "source": [
    "# OpenDengue export columns\n",
    "DATE_COL = \"calendar_start_date\"\n",
    "CASES_COL = \"dengue_total\"\n",
    "ADMIN2_COL = \"adm_2_name\"\n",
    "\n",
    "missing = [c for c in [DATE_COL, CASES_COL, ADMIN2_COL] if c not in df_raw.columns]\n",
    "if missing:\n",
    "    raise KeyError(\n",
    "        f\"Input CSV is missing required columns: {missing}. \"\n",
    "        f\"Available columns: {df_raw.columns.tolist()}\"\n",
    "    )\n",
    "\n",
    "print(\"Using columns:\", {\"date\": DATE_COL, \"cases\": CASES_COL, \"admin2\": ADMIN2_COL})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5bf4c5",
   "metadata": {},
   "source": [
    "## Normalize OpenDengue (Nepal districts / admin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56407342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmapped dengue rows: 100.00%\n",
      "Examples: ['NAN', 'BARDIYA', 'BAITADI', 'KAILALI', 'SURKHET', 'MAKAWANPUR', 'CHITWAN', 'BANKE', 'SARLAHI', 'PYUTHAN', 'MORANG', 'SYANGJA', 'DHADING', 'RUKUM', 'KANCHANPUR']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>cases</th>\n",
       "      <th>district_name</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date, cases, district_name, location]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_norm = pd.DataFrame({\n",
    "    \"date\": pd.to_datetime(df_raw[DATE_COL], errors=\"coerce\"),\n",
    "    \"cases\": pd.to_numeric(df_raw[CASES_COL], errors=\"coerce\"),\n",
    "    \"district_name\": df_raw[ADMIN2_COL],   # <-- not location yet\n",
    "})\n",
    "\n",
    "# Normalize district name for the crosswalk join\n",
    "df_norm[\"district_name\"] = (\n",
    "    df_norm[\"district_name\"]\n",
    "    .astype(str)\n",
    "    .str.upper()\n",
    "    .str.strip()\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    ")\n",
    "\n",
    "# Keep only valid rows\n",
    "# Map district_name -> DHIS2 orgUnit UID\n",
    "df_norm = df_norm.merge(\n",
    "    locations[[\"district_name\", \"location\"]],\n",
    "    on=\"district_name\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Fail fast (or drop) if mapping is incomplete\n",
    "unmapped = df_norm[\"location\"].isna().mean()\n",
    "print(f\"Unmapped dengue rows: {unmapped:.2%}\")\n",
    "if unmapped > 0:\n",
    "    print(\"Examples:\", df_norm.loc[df_norm[\"location\"].isna(), \"district_name\"].drop_duplicates().head(15).tolist())\n",
    "\n",
    "df_norm = df_norm.dropna(subset=[\"location\"]).copy()\n",
    "\n",
    "\n",
    "df_norm = df_norm.dropna(subset=[\"date\", \"cases\", \"district_name\"])\n",
    "df_norm = df_norm[df_norm[\"district_name\"].ne(\"\")]\n",
    "\n",
    "df_norm.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8657935",
   "metadata": {},
   "source": [
    "## Monthly aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20572e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated rows: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_period</th>\n",
       "      <th>location</th>\n",
       "      <th>disease_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [time_period, location, disease_cases]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to month period label (YYYY-MM)\n",
    "df_norm[\"time_period\"] = df_norm[\"date\"].dt.to_period(\"M\").astype(str)\n",
    "\n",
    "# Aggregate within month + location\n",
    "disease = (\n",
    "    df_norm.groupby([\"time_period\", \"location\"], as_index=False)[\"cases\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"cases\": \"disease_cases\"})\n",
    ")\n",
    "\n",
    "print(\"Aggregated rows:\", len(disease))\n",
    "disease.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec13e34",
   "metadata": {},
   "source": [
    "## Filter to spatial backbone and align time axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53c14efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone filter kept 0/0 rows\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "start and end must not be NaT",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBackbone filter kept \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mafter\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbefore\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Build full (time_period x location) grid and fill missing with 0\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m all_months = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mperiod_range\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisease\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtime_period\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisease\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtime_period\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mM\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.astype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m      9\u001b[39m all_locations = locations[\u001b[33m\"\u001b[39m\u001b[33mlocation\u001b[39m\u001b[33m\"\u001b[39m].sort_values().unique()\n\u001b[32m     11\u001b[39m grid = pd.MultiIndex.from_product([all_months, all_locations], names=[\u001b[33m\"\u001b[39m\u001b[33mtime_period\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlocation\u001b[39m\u001b[33m\"\u001b[39m]).to_frame(index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/CLIMATE_HEALTH/climate-tools/.venv/lib/python3.12/site-packages/pandas/core/indexes/period.py:611\u001b[39m, in \u001b[36mperiod_range\u001b[39m\u001b[34m(start, end, periods, freq, name)\u001b[39m\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m freq \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(start, Period) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(end, Period)):\n\u001b[32m    609\u001b[39m     freq = \u001b[33m\"\u001b[39m\u001b[33mD\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m data, freq = \u001b[43mPeriodArray\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_generate_range\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    612\u001b[39m dtype = PeriodDtype(freq)\n\u001b[32m    613\u001b[39m data = PeriodArray(data, dtype=dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/CLIMATE_HEALTH/climate-tools/.venv/lib/python3.12/site-packages/pandas/core/arrays/period.py:343\u001b[39m, in \u001b[36mPeriodArray._generate_range\u001b[39m\u001b[34m(cls, start, end, periods, freq)\u001b[39m\n\u001b[32m    340\u001b[39m     freq = Period._maybe_convert_freq(freq)\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m end \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     subarr, freq = \u001b[43m_get_ordinal_range\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNot enough parameters to construct Period range\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/CLIMATE_HEALTH/climate-tools/.venv/lib/python3.12/site-packages/pandas/core/arrays/period.py:1238\u001b[39m, in \u001b[36m_get_ordinal_range\u001b[39m\u001b[34m(start, end, periods, freq, mult)\u001b[39m\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mstart and end must have same freq\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;129;01mis\u001b[39;00m NaT \u001b[38;5;129;01mor\u001b[39;00m end \u001b[38;5;129;01mis\u001b[39;00m NaT:\n\u001b[32m-> \u001b[39m\u001b[32m1238\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mstart and end must not be NaT\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m freq \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_start_per:\n",
      "\u001b[31mValueError\u001b[39m: start and end must not be NaT"
     ]
    }
   ],
   "source": [
    "# Keep only locations present in the GeoJSON backbone\n",
    "before = len(disease)\n",
    "disease = disease.merge(locations[[\"location\"]], on=\"location\", how=\"inner\")\n",
    "after = len(disease)\n",
    "print(f\"Backbone filter kept {after}/{before} rows\")\n",
    "\n",
    "# Build full (time_period x location) grid and fill missing with 0\n",
    "all_months = pd.period_range(disease[\"time_period\"].min(), disease[\"time_period\"].max(), freq=\"M\").astype(str)\n",
    "all_locations = locations[\"location\"].sort_values().unique()\n",
    "\n",
    "grid = pd.MultiIndex.from_product([all_months, all_locations], names=[\"time_period\", \"location\"]).to_frame(index=False)\n",
    "\n",
    "disease_full = grid.merge(disease, on=[\"time_period\", \"location\"], how=\"left\")\n",
    "disease_full[\"disease_cases\"] = disease_full[\"disease_cases\"].fillna(0)\n",
    "\n",
    "# Keep integer-looking values as ints where possible\n",
    "disease_full[\"disease_cases\"] = pd.to_numeric(disease_full[\"disease_cases\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "print(\"Final rows (complete grid):\", len(disease_full))\n",
    "disease_full.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526a22ae",
   "metadata": {},
   "source": [
    "## Write output CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02edd99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_full.to_csv(OUT_CSV, index=False)\n",
    "print(\"Wrote:\", OUT_CSV)\n",
    "OUT_CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c4c779",
   "metadata": {},
   "source": [
    "## Import into DHIS2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f3b8f8",
   "metadata": {},
   "source": [
    "This workflow stops after producing a harmonized, DHIS2-ready dataset.\n",
    "\n",
    "To import the resulting data into DHIS2:\n",
    "\n",
    "- create a data element for dengue case counts\n",
    "- map locations to DHIS2 organisation units\n",
    "- submit the data using the DHIS2 Web API\n",
    "\n",
    "The import mechanics are identical to those used in the WorldPop and CHIRPS workflows and are not repeated here.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
