{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217827ea",
   "metadata": {},
   "source": [
    "---\n",
    "title: Full workflow for importing Dengue data into DHIS2\n",
    "short_title: Import Dengue Data\n",
    "---\n",
    "\n",
    "This workflow demonstrates the end-to-end preparation of importing dengue case data into DHIS2. We demonstrate the workflow using [**OpenDengue**](https://opendengue.org/data.html) data, otherwise it is expected for countries to use official Ministry of Health data.\n",
    "\n",
    "The notebook focuses on **data harmonization and preparation** using a worked example for **Nepal (districts / admin2)** and **monthly** data. The final DHIS2 import step follows the same approach as the WorldPop and CHIRPS workflows and is therefore not repeated in full here.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "This workflow expects two local input files under `../../guides/data/`:\n",
    "\n",
    "- `nepal-opendengue.csv` — [**OpenDengue**](https://opendengue.org/data.html) export containing Nepal dengue case counts\n",
    "- `nepal-locations.geojson` — Nepal district geometries (admin2)\n",
    "\n",
    "## Output\n",
    "\n",
    "The workflow produces:\n",
    "\n",
    "- `nepal-dengue-harmonized.csv` — harmonized monthly dengue cases per district (`time_period`, `location`, `disease_cases`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ff57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d837eb6",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b539e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root folder\n",
    "DATA_FOLDER = Path(\"../../guides/data\")\n",
    "\n",
    "LOCATIONS_GEOJSON = DATA_FOLDER / \"nepal-locations.geojson\"\n",
    "OPENDENGUE_SOURCE_PATH = DATA_FOLDER / \"nepal-opendengue.csv\"\n",
    "\n",
    "# Output\n",
    "OUT_CSV = DATA_FOLDER / \"nepal-dengue-harmonized.csv\"\n",
    "\n",
    "for p in [LOCATIONS_GEOJSON, OPENDENGUE_SOURCE_PATH]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing required input: {p}\")\n",
    "\n",
    "print(\"Using inputs:\")\n",
    "print(\" -\", LOCATIONS_GEOJSON)\n",
    "print(\" -\", OPENDENGUE_SOURCE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa838c57",
   "metadata": {},
   "source": [
    "## Load district locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f613a13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "districts: 77\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>name_raw</th>\n",
       "      <th>district_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BdLcDbLQd88</td>\n",
       "      <td>101 TAPLEJUNG</td>\n",
       "      <td>101 TAPLEJUNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uHEl9oRZm8L</td>\n",
       "      <td>102 SANKHUWASABHA</td>\n",
       "      <td>102 SANKHUWASABHA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wep3D4POB3H</td>\n",
       "      <td>103 SOLUKHUMBU</td>\n",
       "      <td>103 SOLUKHUMBU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B7X957nA1lM</td>\n",
       "      <td>104 OKHALDHUNGA</td>\n",
       "      <td>104 OKHALDHUNGA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LnJ8MTOmgGa</td>\n",
       "      <td>105 KHOTANG</td>\n",
       "      <td>105 KHOTANG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      location           name_raw      district_name\n",
       "0  BdLcDbLQd88      101 TAPLEJUNG      101 TAPLEJUNG\n",
       "1  uHEl9oRZm8L  102 SANKHUWASABHA  102 SANKHUWASABHA\n",
       "2  Wep3D4POB3H     103 SOLUKHUMBU     103 SOLUKHUMBU\n",
       "3  B7X957nA1lM    104 OKHALDHUNGA    104 OKHALDHUNGA\n",
       "4  LnJ8MTOmgGa        105 KHOTANG        105 KHOTANG"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def norm_name(s: pd.Series) -> pd.Series:\n",
    "    return (\n",
    "        s.astype(str)\n",
    "         .str.upper()\n",
    "         .str.strip()\n",
    "         .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    )\n",
    "\n",
    "with open(LOCATIONS_GEOJSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    gj = json.load(f)\n",
    "\n",
    "districts = pd.DataFrame([{\n",
    "    \"location\": feat.get(\"id\"),                      # DHIS2 level-2 orgUnit UID\n",
    "    \"name_raw\": feat[\"properties\"].get(\"name\", \"\"),  # e.g. \"101 TAPLEJUNG\"\n",
    "} for feat in gj[\"features\"]])\n",
    "\n",
    "districts[\"district_name\"] = norm_name(districts[\"name_raw\"])\n",
    "\n",
    "# sanity\n",
    "assert districts[\"location\"].notna().all()\n",
    "assert districts[\"district_name\"].notna().all()\n",
    "assert not districts.duplicated(\"district_name\").any()\n",
    "\n",
    "print(\"districts:\", len(districts))\n",
    "districts.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb287b",
   "metadata": {},
   "source": [
    "## Load OpenDengue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d8490bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: ../../guides/data/nepal-opendengue.csv\n",
      "Columns: ['adm_0_name', 'adm_1_name', 'adm_2_name', 'full_name', 'ISO_A0', 'FAO_GAUL_code', 'RNE_iso_code', 'IBGE_code', 'calendar_start_date', 'calendar_end_date', 'Year', 'dengue_total', 'case_definition_standardised', 'S_res', 'T_res', 'UUID', 'region']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adm_0_name</th>\n",
       "      <th>adm_1_name</th>\n",
       "      <th>adm_2_name</th>\n",
       "      <th>full_name</th>\n",
       "      <th>ISO_A0</th>\n",
       "      <th>FAO_GAUL_code</th>\n",
       "      <th>RNE_iso_code</th>\n",
       "      <th>IBGE_code</th>\n",
       "      <th>calendar_start_date</th>\n",
       "      <th>calendar_end_date</th>\n",
       "      <th>Year</th>\n",
       "      <th>dengue_total</th>\n",
       "      <th>case_definition_standardised</th>\n",
       "      <th>S_res</th>\n",
       "      <th>T_res</th>\n",
       "      <th>UUID</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEPAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEPAL</td>\n",
       "      <td>NPL</td>\n",
       "      <td>175</td>\n",
       "      <td>NPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1987-01-01</td>\n",
       "      <td>1987-12-31</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>Total</td>\n",
       "      <td>Admin0</td>\n",
       "      <td>Year</td>\n",
       "      <td>WHOSEARO-ALL-19852009-Y01-00</td>\n",
       "      <td>SEARO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NEPAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEPAL</td>\n",
       "      <td>NPL</td>\n",
       "      <td>175</td>\n",
       "      <td>NPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1985-01-01</td>\n",
       "      <td>1985-12-31</td>\n",
       "      <td>1985</td>\n",
       "      <td>0</td>\n",
       "      <td>Total</td>\n",
       "      <td>Admin0</td>\n",
       "      <td>Year</td>\n",
       "      <td>WHOSEARO-ALL-19852009-Y01-00</td>\n",
       "      <td>SEARO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEPAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEPAL</td>\n",
       "      <td>NPL</td>\n",
       "      <td>175</td>\n",
       "      <td>NPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1986-01-01</td>\n",
       "      <td>1986-12-31</td>\n",
       "      <td>1986</td>\n",
       "      <td>0</td>\n",
       "      <td>Total</td>\n",
       "      <td>Admin0</td>\n",
       "      <td>Year</td>\n",
       "      <td>WHOSEARO-ALL-19852009-Y01-00</td>\n",
       "      <td>SEARO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEPAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEPAL</td>\n",
       "      <td>NPL</td>\n",
       "      <td>175</td>\n",
       "      <td>NPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991-01-01</td>\n",
       "      <td>1991-12-31</td>\n",
       "      <td>1991</td>\n",
       "      <td>0</td>\n",
       "      <td>Total</td>\n",
       "      <td>Admin0</td>\n",
       "      <td>Year</td>\n",
       "      <td>WHOSEARO-ALL-19852009-Y01-00</td>\n",
       "      <td>SEARO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEPAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEPAL</td>\n",
       "      <td>NPL</td>\n",
       "      <td>175</td>\n",
       "      <td>NPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1988-01-01</td>\n",
       "      <td>1988-12-31</td>\n",
       "      <td>1988</td>\n",
       "      <td>0</td>\n",
       "      <td>Total</td>\n",
       "      <td>Admin0</td>\n",
       "      <td>Year</td>\n",
       "      <td>WHOSEARO-ALL-19852009-Y01-00</td>\n",
       "      <td>SEARO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  adm_0_name adm_1_name adm_2_name full_name ISO_A0  FAO_GAUL_code  \\\n",
       "0      NEPAL        NaN        NaN     NEPAL    NPL            175   \n",
       "1      NEPAL        NaN        NaN     NEPAL    NPL            175   \n",
       "2      NEPAL        NaN        NaN     NEPAL    NPL            175   \n",
       "3      NEPAL        NaN        NaN     NEPAL    NPL            175   \n",
       "4      NEPAL        NaN        NaN     NEPAL    NPL            175   \n",
       "\n",
       "  RNE_iso_code  IBGE_code calendar_start_date calendar_end_date  Year  \\\n",
       "0          NPL        NaN          1987-01-01        1987-12-31  1987   \n",
       "1          NPL        NaN          1985-01-01        1985-12-31  1985   \n",
       "2          NPL        NaN          1986-01-01        1986-12-31  1986   \n",
       "3          NPL        NaN          1991-01-01        1991-12-31  1991   \n",
       "4          NPL        NaN          1988-01-01        1988-12-31  1988   \n",
       "\n",
       "   dengue_total case_definition_standardised   S_res T_res  \\\n",
       "0             0                        Total  Admin0  Year   \n",
       "1             0                        Total  Admin0  Year   \n",
       "2             0                        Total  Admin0  Year   \n",
       "3             0                        Total  Admin0  Year   \n",
       "4             0                        Total  Admin0  Year   \n",
       "\n",
       "                           UUID region  \n",
       "0  WHOSEARO-ALL-19852009-Y01-00  SEARO  \n",
       "1  WHOSEARO-ALL-19852009-Y01-00  SEARO  \n",
       "2  WHOSEARO-ALL-19852009-Y01-00  SEARO  \n",
       "3  WHOSEARO-ALL-19852009-Y01-00  SEARO  \n",
       "4  WHOSEARO-ALL-19852009-Y01-00  SEARO  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv(OPENDENGUE_SOURCE_PATH)\n",
    "print(\"Loaded:\", OPENDENGUE_SOURCE_PATH)\n",
    "print(\"Columns:\", df_raw.columns.tolist())\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db9e6bd",
   "metadata": {},
   "source": [
    "## Column mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b1673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenDengue export columns (Nepal example)\n",
    "DATE_COL = \"calendar_start_date\"\n",
    "CASES_COL = \"dengue_total\"\n",
    "ADMIN2_COL = \"adm_2_name\"\n",
    "\n",
    "missing = [c for c in [DATE_COL, CASES_COL, ADMIN2_COL] if c not in df_raw.columns]\n",
    "if missing:\n",
    "    raise KeyError(\n",
    "        f\"Input CSV is missing required columns: {missing}. \"\n",
    "        f\"Available columns: {df_raw.columns.tolist()}\"\n",
    "    )\n",
    "\n",
    "print(\"Using columns:\", {\"date\": DATE_COL, \"cases\": CASES_COL, \"admin2\": ADMIN2_COL})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5bf4c5",
   "metadata": {},
   "source": [
    "## Normalize OpenDengue (Nepal districts / admin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56407342",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = pd.DataFrame({\n",
    "    \"date\": pd.to_datetime(df_raw[DATE_COL], errors=\"coerce\"),\n",
    "    \"cases\": pd.to_numeric(df_raw[CASES_COL], errors=\"coerce\"),\n",
    "    \"district_name\": df_raw[ADMIN2_COL],\n",
    "})\n",
    "\n",
    "df_norm[\"district_name\"] = norm_name(df_norm[\"district_name\"])\n",
    "\n",
    "df_norm = df_norm.dropna(subset=[\"date\", \"cases\", \"district_name\"])\n",
    "df_norm = df_norm[df_norm[\"district_name\"].ne(\"\")]\n",
    "\n",
    "# Map district_name -> DHIS2 UID (location)\n",
    "df_norm = df_norm.merge(\n",
    "    districts[[\"district_name\", \"location\"]],\n",
    "    on=\"district_name\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "unmapped = df_norm[\"location\"].isna().mean()\n",
    "print(f\"Unmapped dengue rows: {unmapped:.2%}\")\n",
    "if unmapped > 0:\n",
    "    print(\"Unmapped examples:\", df_norm.loc[df_norm[\"location\"].isna(), \"district_name\"].drop_duplicates().head(20).tolist())\n",
    "\n",
    "# Keep only mapped rows (otherwise they will never join downstream)\n",
    "df_norm = df_norm.dropna(subset=[\"location\"]).copy()\n",
    "\n",
    "df_norm.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8657935",
   "metadata": {},
   "source": [
    "## Monthly aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20572e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm[\"time_period\"] = (\n",
    "    df_norm[\"date\"]\n",
    "    .dt.to_period(\"M\")\n",
    "    .astype(str)\n",
    "    .str.replace(\"-\", \"\", regex=False)  # YYYYMM\n",
    ")\n",
    "\n",
    "disease = (\n",
    "    df_norm.groupby([\"time_period\", \"location\"], as_index=False)[\"cases\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"cases\": \"disease_cases\"})\n",
    ")\n",
    "\n",
    "print(\"Aggregated rows:\", len(disease))\n",
    "disease.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec13e34",
   "metadata": {},
   "source": [
    "## Filter to districts and align time axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c14efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only districts present in the DHIS2 UID\n",
    "before = len(disease)\n",
    "disease = disease.merge(districts[[\"location\"]].drop_duplicates(), on=\"location\", how=\"inner\")\n",
    "after = len(disease)\n",
    "print(f\"Districts  kept {after}/{before} rows\")\n",
    "\n",
    "# Build full (time_period x location) grid — preserve missing as NaN (no imputation)\n",
    "all_months = pd.Index(\n",
    "    pd.period_range(\n",
    "        pd.Period(disease[\"time_period\"].min(), freq=\"M\").to_timestamp(),\n",
    "        pd.Period(disease[\"time_period\"].max(), freq=\"M\").to_timestamp(),\n",
    "        freq=\"M\",\n",
    "    ).astype(str).str.replace(\"-\", \"\", regex=False),  # YYYYMM\n",
    "    name=\"time_period\",\n",
    ")\n",
    "\n",
    "all_locations = pd.Index(districts[\"location\"].dropna().astype(str).sort_values().unique(), name=\"location\")\n",
    "\n",
    "grid = pd.MultiIndex.from_product([all_months, all_locations], names=[\"time_period\", \"location\"]).to_frame(index=False)\n",
    "\n",
    "disease_full = grid.merge(disease, on=[\"time_period\", \"location\"], how=\"left\")\n",
    "\n",
    "# Preserve missingness; just ensure numeric dtype\n",
    "disease_full[\"disease_cases\"] = pd.to_numeric(disease_full[\"disease_cases\"], errors=\"coerce\")\n",
    "\n",
    "print(\"Final rows (complete grid):\", len(disease_full))\n",
    "disease_full.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526a22ae",
   "metadata": {},
   "source": [
    "## Write output CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02edd99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_full.to_csv(OUT_CSV, index=False)\n",
    "print(\"Wrote:\", OUT_CSV)\n",
    "OUT_CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c4c779",
   "metadata": {},
   "source": [
    "## Import into DHIS2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f3b8f8",
   "metadata": {},
   "source": [
    "This workflow stops after producing a harmonized, DHIS2-ready dataset.\n",
    "\n",
    "To import the resulting data into DHIS2:\n",
    "\n",
    "- create a data element for dengue case counts\n",
    "- map locations to DHIS2 organisation units\n",
    "- submit the data using the DHIS2 Web API\n",
    "\n",
    "The import mechanics are identical to those used in the WorldPop and CHIRPS workflows and are not repeated here.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
