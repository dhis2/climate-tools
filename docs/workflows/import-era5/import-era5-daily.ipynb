{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Full workflow for importing ERA5-Land into DHIS2\"\n",
    "short_title: \"Import ERA5 data\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example workflow, we show a complete workflow that uses DHIS2 Climate Tools to make sure that DHIS2 is continuously updated with the latest climate data. Specifically we create a reusable function which **downloads and imports** the daily precipitation data for a given time period from the [ERA5-Land hourly data](https://cds.climate.copernicus.eu/datasets/reanalysis-era5-land?tab=overview) hosted at the Climate Data Store. \n",
    "\n",
    "By updating the `end_date` to `date.today()` and running the script at regular intervals, DHIS2 can be kept automatically up to date with newly available precipitation data.\n",
    "\n",
    "If you're only interested in downloading ERA5-Land data, see [this detailed step-by-step guide](../../guides/getting-data/climate-data-store/era5-download.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running the code example, make sure the following are in place.\n",
    "\n",
    "### 1. CDS API access\n",
    "\n",
    "Make sure you have followed [these instructions](../../guides/getting-data/climate-data-store/api-authentication.md) to authenticate and allow API access the CDS portal. \n",
    "\n",
    "### 2. Required DHIS2 data element\n",
    "\n",
    "Your DHIS2 instance must contain a data element that can receive the imported data.\n",
    "\n",
    "For daily precipitation, the data element must have:\n",
    "\n",
    "- `valueType = NUMBER`\n",
    "- `aggregationType = SUM`\n",
    "- It must belong to a data set with `periodType = DAILY`\n",
    "\n",
    "If this data element does not already exist, you have two options:\n",
    "\n",
    "- Create the data element manually in DHIS2.\n",
    "- Or create the data element using the Python client by following [this guide](../../guides/import-data/prepare-metadata.ipynb).\n",
    "\n",
    "Once the data element exists, copy its UID and set it as `data_element_id` in the code example in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: If you are using the public DHIS2 demo instance, note that it does not contain the required data element, and that it will reset every night. This means the data element and UID will need to be recreated and updated for each day you run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A reuseable script for importing data\n",
    "\n",
    "In the code sample below, we show an example of how all the steps from the previous guides can be combined to create a single function with parameter inputs that: \n",
    "\n",
    "- Loops through each month between a start and end date\n",
    "- Checks and skips to the next month if the data for the current month has already been imported into DHIS2\n",
    "- Downloads a single climate variable for the current month from [hourly ERA5-Land data](https://cds.climate.copernicus.eu/datasets/reanalysis-era5-land?tab=overview)\n",
    "- Aggregates and processes the data\n",
    "- And finally imports the data into DHIS2\n",
    "\n",
    "You can copy-paste the code below to your own script, and easily change the import parameters to instead import temperature or other ERA5 climate variables, aggregation methods, start/end date, timezone offset, which organisation unit level to import for, etc. \n",
    "\n",
    "> Important: The code below only aggregates to daily periods according the Gregorian calendar. Other calendar systems, like those used in Nepal or Ethiopia, are not yet supported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping field groups: unsupported OGR type: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meta': {'dataElement': 'bMoGyfJoH9c', 'level': 2, 'periodType': 'DAILY', 'calendar': 'iso8601', 'years_checked': 1}, 'existing': {'id': '20251201', 'startDate': '2025-12-01', 'endDate': '2025-12-01'}, 'next': {'id': '20251202', 'startDate': '2025-12-02', 'endDate': '2025-12-02'}}\n",
      "\n",
      "==================\n",
      "Processing 202510\n",
      "Comparing 202510 with 202512, needs import: False\n",
      "All data already imported for this month\n",
      "\n",
      "==================\n",
      "Processing 202511\n",
      "Comparing 202511 with 202512, needs import: False\n",
      "All data already imported for this month\n",
      "\n",
      "==================\n",
      "Processing 202512\n",
      "Comparing 202512 with 202512, needs import: True\n",
      "Downloading data...\n",
      "dhis2eo.data.utils - INFO - Loading from cache: C:\\Users\\karimba\\AppData\\Local\\Temp\\dhis2eo_data_cds_era5_land_hourly_get_788c2eb198.nc\n",
      "Aggregating time...\n",
      "Aggregating to org units...\n",
      "Post-processing...\n",
      "Creating payload...\n",
      "Importing...\n",
      "--> Import results: {'imported': 0, 'updated': 0, 'ignored': 65, 'deleted': 0}\n"
     ]
    }
   ],
   "source": [
    "from dhis2_client import DHIS2Client\n",
    "\n",
    "from dhis2eo import utils\n",
    "from dhis2eo.data.cds import era5_land\n",
    "from dhis2eo.integrations.pandas import dataframe_to_dhis2_json\n",
    "\n",
    "from earthkit import transforms\n",
    "import geopandas as gpd\n",
    "\n",
    "import json\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "################\n",
    "# Script Inputs:\n",
    "\n",
    "# connect to DHIS2\n",
    "client = DHIS2Client(\n",
    "    base_url=\"https://play.im.dhis2.org/stable-2-42-3-1\",\n",
    "    username=\"admin\",\n",
    "    password=\"district\",\n",
    ")\n",
    "\n",
    "# define the ERA5 variable to import\n",
    "variable = 'total_precipitation'  # choose a variable you want to download and go to the \"API request\"\n",
    "\n",
    "# define which DHIS2 data element to import data into\n",
    "data_element_id = 'bMoGyfJoH9c'  # update this to the correct DHIS2 data element\n",
    "\n",
    "# define how to get and process the values\n",
    "value_col = 'tp'   # name of the column that contains the values\n",
    "def value_processing(value):\n",
    "    # for this example we convert precipitation from m to mm\n",
    "    # but you should change this function for other variables\n",
    "    return value * 1000\n",
    "\n",
    "# define how to aggregate the values\n",
    "temporal_aggregation = 'sum'  # this is for total precipitation, but other variables like temperature should be set to 'mean'\n",
    "spatial_aggregation = 'mean'  # this should almost always be 'mean'\n",
    "\n",
    "# define the start and end dates\n",
    "# for now, this should be gregorian calendar dates only\n",
    "start_date = '2025-10-01'\n",
    "end_date = '2025-12-30'\n",
    "\n",
    "# offset the hourly data from UTC to your local timezone\n",
    "timezone_offset = 0  # Sierra Leone is UTC+0, so we use an offset of 0\n",
    "\n",
    "# which level of organisation unit to import data into\n",
    "org_unit_level = 2\n",
    "\n",
    "\n",
    "###########################\n",
    "# Define Reusable Function:\n",
    "\n",
    "def import_era5_land_to_dhis2(client, variable, data_element_id, \n",
    "                              value_col, value_func, \n",
    "                              temporal_aggregation, spatial_aggregation,\n",
    "                              start_date, end_date, timezone_offset, \n",
    "                              org_unit_level, dry_run=False): \n",
    "    # define the era5 variable names to download\n",
    "    variables = [variable]\n",
    "\n",
    "    # parse start and end month\n",
    "    start_date,end_date = date.fromisoformat(start_date), date.fromisoformat(end_date)\n",
    "    start_year,start_month = start_date.year, start_date.month\n",
    "    end_year,end_month = end_date.year, end_date.month\n",
    "\n",
    "    # get org units from DHIS2\n",
    "    org_units_geojson = client.get_org_units_geojson(level=org_unit_level)\n",
    "    org_units = gpd.read_file(json.dumps(org_units_geojson))\n",
    "\n",
    "    # get last imported month, and convert to string to use for comparisons\n",
    "    # the results contains an `existing` entry which contains information about the last imported period \n",
    "    # ...for which data was found, or `None` if no existing data was found\n",
    "    last_imported_response = client.analytics_latest_period_for_level(de_uid=data_element_id, level=org_unit_level)\n",
    "    print(last_imported_response)\n",
    "    last_imported_period = last_imported_response['existing']\n",
    "    last_imported_month_string = last_imported_period['id'][:6] if last_imported_period else None\n",
    "\n",
    "    # loop through and process one month at a time\n",
    "    for year,month in utils.time.iter_months(start_year, start_month, end_year, end_month):\n",
    "        month_string = utils.time.dhis2_period(year=year, month=month)\n",
    "        print('')\n",
    "        print('==================')\n",
    "        print(f'Processing {month_string}')\n",
    "\n",
    "        # determine whether data for this month has already been imported\n",
    "        # we still import if this month is the latest imported one (to allow updates to partially imported months)\n",
    "        needs_import = last_imported_month_string is None or (month_string >= last_imported_month_string)\n",
    "        print(f'Comparing {month_string} with {last_imported_month_string}, needs import: {needs_import}')\n",
    "\n",
    "        # only continue if some of the data needs importing\n",
    "        if not needs_import:\n",
    "            print('All data already imported for this month')\n",
    "            continue\n",
    "\n",
    "        # download era5 data\n",
    "        print('Downloading data...')\n",
    "        hourly_data = era5_land.hourly.get(year=year, month=month, variables=variables, bbox=org_units.total_bounds)\n",
    "\n",
    "        # aggregate to time period\n",
    "        print('Aggregating time...')\n",
    "        agg_time = transforms.temporal.daily_reduce(\n",
    "            hourly_data[value_col], \n",
    "            how=temporal_aggregation, \n",
    "            time_shift={'hours': timezone_offset},\n",
    "            remove_partial_periods=False,\n",
    "        )\n",
    "\n",
    "        # aggregate to org units\n",
    "        print('Aggregating to org units...')\n",
    "        agg_org_units = transforms.spatial.reduce(\n",
    "            agg_time, org_units, \n",
    "            mask_dim='id', \n",
    "            how=spatial_aggregation,\n",
    "        )\n",
    "        agg_df = agg_org_units.to_dataframe().reset_index()\n",
    "\n",
    "        # post-processing\n",
    "        print('Post-processing...')\n",
    "        agg_df[value_col] = agg_df[value_col].apply(value_func)\n",
    "\n",
    "        # create json payload\n",
    "        print('Creating payload...')\n",
    "        payload = dataframe_to_dhis2_json(\n",
    "            df=agg_df,\n",
    "            org_unit_col='id',\n",
    "            period_col='valid_time',\n",
    "            value_col=value_col,\n",
    "            data_element_id=data_element_id,\n",
    "        )\n",
    "        \n",
    "        # import to dhis2\n",
    "        print('Importing...')\n",
    "        res = client.post(\"/api/dataValueSets\", json=payload, params={\"dryRun\": str(dry_run).lower()})\n",
    "        print(f'--> Import results: {res['response']['importCount']}')\n",
    "\n",
    "\n",
    "##########################\n",
    "# Run the import function:\n",
    "\n",
    "import_era5_land_to_dhis2(client, \n",
    "                            variable=variable, data_element_id=data_element_id, \n",
    "                            value_col=value_col, value_func=value_processing, \n",
    "                            temporal_aggregation=temporal_aggregation, spatial_aggregation=spatial_aggregation,\n",
    "                            start_date=start_date, end_date=end_date, timezone_offset=timezone_offset, \n",
    "                            org_unit_level=org_unit_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate-tools-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
